#!/usr/bin/python3
# ugrep: find unicode characters based on their names or number.
# 	 Essentially grep for the Unicode table.

# PREREQUISITE: 
#
# * You'll need a copy of UnicodeData.txt installed. 
#   On Debian GNU/Linux, this can be done by `apt install unicode-data`.
#   Or, you can download it by hand from the Unicode Consortium and place
#   it in `~/.local/share/unicode/UnicodeData.txt`.

# ADDED FUNCTIONALITY: 
#
# * If you have FontConfig installed, then you can use -l to see which
#   fonts contain a certain character.
#
# * If you have ImageMagick installed and your terminal can display
#   sixels, then -l will also show a rendering of the character in
#   each font.


# Fun things to try:

# ugrep alchemical 
# ugrep ornament
# ugrep bullet
# ugrep 'vine|bud'
# ugrep vai
# ugrep heavy
# ugrep drawing
# ugrep 2300..ff

# GPL â‰¥3 (see LICENSE file)
# B9 September 2018 â€“ October 2021

debug=False

from re import *
from sys import argv, stderr
from os.path import expanduser, expandvars, basename
from os import isatty
from subprocess import getoutput, run
from posix import write
import termios
import argparse

def usage():
    print("""\
ugrep: find unicode characters based on their names or codepoints
Usage: 
    ugrep [-wlx] [-L n] <character> | <codepoint ...> | <character name>
    ugrep -c <character string ...>

              -w:  Match only whole words:		ugrep -w pi
              -c:  Show each character in a string:	ugrep -c "(ï¾Ÿâˆ€ï¾Ÿ)"
              -l:  List installed fonts that include matching characters.
             	   -ll: Also includes style variants (bold, italic,...)
        	   -L scale:  Scale font examples from 2 to 8x.
              -x:  Show example usage.

       <character> A single character; implies -c:	ugrep â˜™

       <codepoint> One or more hexadecimal numbers:	ugrep U+1F639
		   Ranges are allowed with two dots:	ugrep 23b0..f

  <character name> A Unicode name (as a regex), e.g.:	ugrep alchemical

<character string> One or more characters. e.g.: 	ugrep -c "( Í¡Â° ÍœÊ– Í¡Â°)"

Try 'ugrep --examples' to see typical usage examples.
""", end='' )

def examples():
    print("""\
# You can search by character name.
    $ ugrep heart
    â˜™	U+2619	REVERSED ROTATED FLORAL HEART BULLET
    â£	U+2763	HEAVY HEART EXCLAMATION MARK ORNAMENT
    â¤	U+2764	HEAVY BLACK HEART
    [ ... examples truncated for brevity ... ]
    ðŸ˜»	U+1F63B	SMILING CAT FACE WITH HEART-SHAPED EYES

# Or, you can search by pasting in a specific character.
    $ ugrep âœ¿
    âœ¿       U+273F  BLACK FLORETTE

# Or, you can search by code point.
    $ ugrep 273a
    âœº       U+273A  SIXTEEN POINTED ASTERISK

# By default, words match anywhere.
    $ ugrep clos brac			# Equivalent to "clos.*brac"
    ]       U+005D  RIGHT SQUARE BRACKET (closing square bracket)
    }       U+007D  RIGHT CURLY BRACKET (closing curly bracket)
    ã€‰      U+3009  RIGHT ANGLE BRACKET (closing angle bracket)

# Use -w to match only whole words.
    $ ugrep -w "R"			# Equivalent to "\\bR\\b"
    R	U+0052  LATIN CAPITAL LETTER R
    â„›	U+211B  SCRIPT CAPITAL R (Script r)
    â„œ	U+211C  BLACK-LETTER CAPITAL R (Black-letter r)
    â„	U+211D  DOUBLE-STRUCK CAPITAL R (Double-struck r)

# Use -c to display info for each character in a string.
    $ ugrep -c "á••( á› )á•—"
    á••	U+1555	CANADIAN SYLLABICS FI
    (	U+0028	LEFT PARENTHESIS (opening parenthesis)
    á›	U+141B	CANADIAN SYLLABICS NASKAPI WAA
    )	U+0029	RIGHT PARENTHESIS (closing parenthesis)
    á•—	U+1557	CANADIAN SYLLABICS FO

# ugrep finds aliases (in parens)
    $ ugrep backslash
    \	U+005C	REVERSE SOLIDUS (backslash)

# Regex ^ and $ work, mostly
    $ ugrep ^x				
    âŠ»	U+22BB	XOR
    âŒ§	U+2327	X IN A RECTANGLE BOX (clear key)

# Use -l to see which fonts are installed that can render a certain character
    $ ugrep -l math left tortoise
    âŸ¬    U+27EC  MATHEMATICAL LEFT WHITE TORTOISE SHELL BRACKET
                 Symbola 
                 Unifont 

# Use -L n to scale the font examples n-times larger; implies -l
    $ ugrep -L2 antimony
    [ Cannot show example here ]

# Show every single Unicode character.
    $ ugrep '.?'  |  less"""
    )


# Main
def main():
    global argv, args, debug

    # Load the Unicode Data into the global ucd variable.
    loaducd()
    
    if len(argv) == 1:
        usage()                 # ArgParse's usage is ugly, use our own.
        exit(1)

    if argv[1]=='-h' or argv[1]=='--help':
        usage()
        exit(0)

    if argv[1]=='-x' or argv[1]=='--examples' or argv[1]=='--example':
        examples()
        exit(0)

    parser = argparse.ArgumentParser(
        description='find Unicode characters by name, number, or example.')
    parser.add_argument(
        'characters', metavar='Character Name | Codepoint | Character', 
        nargs='+', type=str,
        help='which character to show specified by name, codepoint or literal example')
    parser.add_argument('-w', '--word-regexp', action="store_true",
        help='matches only whole words, e.g., ugrep -w pi')
    parser.add_argument('-c', '--char-by-char', action="store_true",
        help='show each character in a string, e.g., ugrep -c "(ï¾Ÿâˆ€ï¾Ÿ)"')
    parser.add_argument('-l', '--list-fonts', action="store_true",
        help='list fonts that include a glyph for a given character')
    parser.add_argument('-ll', '--list-fonts-long', action="store_true",
        help='list all fonts, including variants like italic, bold, etc.')
    # XXX Argh. argparse gives an error on 'ugrep -L foo' but works for 'ugrep foo -L'.
    parser.add_argument('-L', '--list-fonts-scale', metavar='scaling', dest='fonts_scale',
                        nargs='?', const=2, default=None, type=float,
        help='list fonts, scaling the characters to be from 2 to 8x larger')
    parser.add_argument('--never-double-text', action="store_true",
        help="Don't use double-size terminal text even when font scale >= 2")
    parser.add_argument('--pipe-sixels', action="store_true",
        help="Allow sixel escape sequences when redirecting stdout")
    parser.add_argument('-x', '--examples',  action="store_true",
        help='show example usage')
    parser.add_argument('--debug', action="store_true", help='internal debugging')

    args = parser.parse_args()

    # -ll (list_fonts_long) same as -l but shows all font variants
    if args.list_fonts_long:
        args.list_fonts = True

    # -l (list fonts), defaults to a scaling of 1x
    if args.list_fonts and not args.fonts_scale:
        args.fonts_scale = 1

    # if -L is like -l (list fonts), but with larger images.
    if args.fonts_scale:
        args.list_fonts = True;

    if args.debug: debug=True

    # If -w, then wrap each argument with \b (regexp word breaks)
    if args.word_regexp:
        args.characters = [ "\\b"+s+"\\b" for s in args.characters ]

    # Is it -c followed by a string of characters?
    if args.char_by_char:
        # For each string s, look up each character in the string.
        for s in args.characters:
            for i in range(len(s)):
                for j in range(0,len(s[i])):
                    showonecharacter( hex(ord(s[i][j])) )
            if s is not args.characters[-1]:
                print("")       # Space between words, if multiple args 
        exit(0)

    # Is it a single character? Then only show that one character.
    if  len(args.characters)==1 and len(args.characters[0])==1:
        showonecharacter( hex(ord(args.characters[0][0])) )
        exit(0)

    # Do range expansion on argv. (e.g., 2301..f)
    args.characters = rangeexpansion(args.characters)

    # Is first arg a hexadecimal number? E.g., 26A0, 0xdeadbeef, U+FACE
    # Then *also* look up that code point.
    if args.characters and match(r"(?i)^(U\+?|0x)?[0-9A-F]+$", args.characters[0]):
        # Loop to allow people to do things like "ugrep 23b0..9"
        for arg in args.characters:
            if match(r"(?i)^(U\+?|0x)?[0-9A-F]+$", arg):
                # Show matches for in Unicode.txt for each hex codepoint  
                if not showonecharacter(arg):
                    # Hmm... it looked like hex, but it wasn't found.
                    # Print the character anyway and its CJK region
                    showcharacterblock(arg)
                    # If -l, show installed fonts which contain the glyph.
                    if args.list_fonts: show_fonts(arg)
            else:
                break

    # Now, look up the entire argv[] as a character name or alias. 
    s=makeregexcharname(args.characters)
    printmatches(s)


def showonecharacter(c):
    s=makeregexcodepoint(c)
    compileit(s)
    return printmatches(s)
              
def showcharacterblock(codepoint):
    """Prints a line showing which character block a hex number belongs in.
    If showonecharacter() fails, this can be used to at least print something,
    """
    x=cp2int(codepoint)         # Remove U+ or 0x prefix and return integer

    block=getblock(x)
    if block:
        try: c=chr(x)
        except ValueError: return
        print("   ", end='') 	# Indent for MacOS
        print(c, end='')
        print('\t' + int2cp(x) + '\t', end='')
        print('Block: [' + block + ']')
        
    
def cp2int(cp):
    """Given a Unicode codepoint as a string (e.g., U+1D0DE)
    convert the hexadecimal number to an integer and return it.
    """
    return( int(cp.lstrip("Uu+0Xx"), 16) )

def int2cp(x):
    "Given an int, return it as a hexadecimal string in the format U+1D0DE)."
    return( "U+%04X" % (x) )

def getblock(x):
    """Given an integer representing a Unicode code point, return its
    block. E.g, U+0041 -> Basic Latin, U+1F675 -> Ornamental Dingbats.
    This is useful to identify characters that are not officially part
    of Unicode, for example all the CJK Ideographs from 4E00 to 9FFF.

    """

    # Data from Unicode 2021 Blocks-14.0.0.txt
    blockstxt="""
0000..007F; Basic Latin
0080..00FF; Latin-1 Supplement
0100..017F; Latin Extended-A
0180..024F; Latin Extended-B
0250..02AF; IPA Extensions
02B0..02FF; Spacing Modifier Letters
0300..036F; Combining Diacritical Marks
0370..03FF; Greek and Coptic
0400..04FF; Cyrillic
0500..052F; Cyrillic Supplement
0530..058F; Armenian
0590..05FF; Hebrew
0600..06FF; Arabic
0700..074F; Syriac
0750..077F; Arabic Supplement
0780..07BF; Thaana
07C0..07FF; NKo
0800..083F; Samaritan
0840..085F; Mandaic
0860..086F; Syriac Supplement
0870..089F; Arabic Extended-B
08A0..08FF; Arabic Extended-A
0900..097F; Devanagari
0980..09FF; Bengali
0A00..0A7F; Gurmukhi
0A80..0AFF; Gujarati
0B00..0B7F; Oriya
0B80..0BFF; Tamil
0C00..0C7F; Telugu
0C80..0CFF; Kannada
0D00..0D7F; Malayalam
0D80..0DFF; Sinhala
0E00..0E7F; Thai
0E80..0EFF; Lao
0F00..0FFF; Tibetan
1000..109F; Myanmar
10A0..10FF; Georgian
1100..11FF; Hangul Jamo
1200..137F; Ethiopic
1380..139F; Ethiopic Supplement
13A0..13FF; Cherokee
1400..167F; Unified Canadian Aboriginal Syllabics
1680..169F; Ogham
16A0..16FF; Runic
1700..171F; Tagalog
1720..173F; Hanunoo
1740..175F; Buhid
1760..177F; Tagbanwa
1780..17FF; Khmer
1800..18AF; Mongolian
18B0..18FF; Unified Canadian Aboriginal Syllabics Extended
1900..194F; Limbu
1950..197F; Tai Le
1980..19DF; New Tai Lue
19E0..19FF; Khmer Symbols
1A00..1A1F; Buginese
1A20..1AAF; Tai Tham
1AB0..1AFF; Combining Diacritical Marks Extended
1B00..1B7F; Balinese
1B80..1BBF; Sundanese
1BC0..1BFF; Batak
1C00..1C4F; Lepcha
1C50..1C7F; Ol Chiki
1C80..1C8F; Cyrillic Extended-C
1C90..1CBF; Georgian Extended
1CC0..1CCF; Sundanese Supplement
1CD0..1CFF; Vedic Extensions
1D00..1D7F; Phonetic Extensions
1D80..1DBF; Phonetic Extensions Supplement
1DC0..1DFF; Combining Diacritical Marks Supplement
1E00..1EFF; Latin Extended Additional
1F00..1FFF; Greek Extended
2000..206F; General Punctuation
2070..209F; Superscripts and Subscripts
20A0..20CF; Currency Symbols
20D0..20FF; Combining Diacritical Marks for Symbols
2100..214F; Letterlike Symbols
2150..218F; Number Forms
2190..21FF; Arrows
2200..22FF; Mathematical Operators
2300..23FF; Miscellaneous Technical
2400..243F; Control Pictures
2440..245F; Optical Character Recognition
2460..24FF; Enclosed Alphanumerics
2500..257F; Box Drawing
2580..259F; Block Elements
25A0..25FF; Geometric Shapes
2600..26FF; Miscellaneous Symbols
2700..27BF; Dingbats
27C0..27EF; Miscellaneous Mathematical Symbols-A
27F0..27FF; Supplemental Arrows-A
2800..28FF; Braille Patterns
2900..297F; Supplemental Arrows-B
2980..29FF; Miscellaneous Mathematical Symbols-B
2A00..2AFF; Supplemental Mathematical Operators
2B00..2BFF; Miscellaneous Symbols and Arrows
2C00..2C5F; Glagolitic
2C60..2C7F; Latin Extended-C
2C80..2CFF; Coptic
2D00..2D2F; Georgian Supplement
2D30..2D7F; Tifinagh
2D80..2DDF; Ethiopic Extended
2DE0..2DFF; Cyrillic Extended-A
2E00..2E7F; Supplemental Punctuation
2E80..2EFF; CJK Radicals Supplement
2F00..2FDF; Kangxi Radicals
2FF0..2FFF; Ideographic Description Characters
3000..303F; CJK Symbols and Punctuation
3040..309F; Hiragana
30A0..30FF; Katakana
3100..312F; Bopomofo
3130..318F; Hangul Compatibility Jamo
3190..319F; Kanbun
31A0..31BF; Bopomofo Extended
31C0..31EF; CJK Strokes
31F0..31FF; Katakana Phonetic Extensions
3200..32FF; Enclosed CJK Letters and Months
3300..33FF; CJK Compatibility
3400..4DBF; CJK Unified Ideographs Extension A
4DC0..4DFF; Yijing Hexagram Symbols
4E00..9FFF; CJK Unified Ideographs
A000..A48F; Yi Syllables
A490..A4CF; Yi Radicals
A4D0..A4FF; Lisu
A500..A63F; Vai
A640..A69F; Cyrillic Extended-B
A6A0..A6FF; Bamum
A700..A71F; Modifier Tone Letters
A720..A7FF; Latin Extended-D
A800..A82F; Syloti Nagri
A830..A83F; Common Indic Number Forms
A840..A87F; Phags-pa
A880..A8DF; Saurashtra
A8E0..A8FF; Devanagari Extended
A900..A92F; Kayah Li
A930..A95F; Rejang
A960..A97F; Hangul Jamo Extended-A
A980..A9DF; Javanese
A9E0..A9FF; Myanmar Extended-B
AA00..AA5F; Cham
AA60..AA7F; Myanmar Extended-A
AA80..AADF; Tai Viet
AAE0..AAFF; Meetei Mayek Extensions
AB00..AB2F; Ethiopic Extended-A
AB30..AB6F; Latin Extended-E
AB70..ABBF; Cherokee Supplement
ABC0..ABFF; Meetei Mayek
AC00..D7AF; Hangul Syllables
D7B0..D7FF; Hangul Jamo Extended-B
D800..DB7F; High Surrogates
DB80..DBFF; High Private Use Surrogates
DC00..DFFF; Low Surrogates
E000..F8FF; Private Use Area
F900..FAFF; CJK Compatibility Ideographs
FB00..FB4F; Alphabetic Presentation Forms
FB50..FDFF; Arabic Presentation Forms-A
FE00..FE0F; Variation Selectors
FE10..FE1F; Vertical Forms
FE20..FE2F; Combining Half Marks
FE30..FE4F; CJK Compatibility Forms
FE50..FE6F; Small Form Variants
FE70..FEFF; Arabic Presentation Forms-B
FF00..FFEF; Halfwidth and Fullwidth Forms
FFF0..FFFF; Specials
10000..1007F; Linear B Syllabary
10080..100FF; Linear B Ideograms
10100..1013F; Aegean Numbers
10140..1018F; Ancient Greek Numbers
10190..101CF; Ancient Symbols
101D0..101FF; Phaistos Disc
10280..1029F; Lycian
102A0..102DF; Carian
102E0..102FF; Coptic Epact Numbers
10300..1032F; Old Italic
10330..1034F; Gothic
10350..1037F; Old Permic
10380..1039F; Ugaritic
103A0..103DF; Old Persian
10400..1044F; Deseret
10450..1047F; Shavian
10480..104AF; Osmanya
104B0..104FF; Osage
10500..1052F; Elbasan
10530..1056F; Caucasian Albanian
10570..105BF; Vithkuqi
10600..1077F; Linear A
10780..107BF; Latin Extended-F
10800..1083F; Cypriot Syllabary
10840..1085F; Imperial Aramaic
10860..1087F; Palmyrene
10880..108AF; Nabataean
108E0..108FF; Hatran
10900..1091F; Phoenician
10920..1093F; Lydian
10980..1099F; Meroitic Hieroglyphs
109A0..109FF; Meroitic Cursive
10A00..10A5F; Kharoshthi
10A60..10A7F; Old South Arabian
10A80..10A9F; Old North Arabian
10AC0..10AFF; Manichaean
10B00..10B3F; Avestan
10B40..10B5F; Inscriptional Parthian
10B60..10B7F; Inscriptional Pahlavi
10B80..10BAF; Psalter Pahlavi
10C00..10C4F; Old Turkic
10C80..10CFF; Old Hungarian
10D00..10D3F; Hanifi Rohingya
10E60..10E7F; Rumi Numeral Symbols
10E80..10EBF; Yezidi
10F00..10F2F; Old Sogdian
10F30..10F6F; Sogdian
10F70..10FAF; Old Uyghur
10FB0..10FDF; Chorasmian
10FE0..10FFF; Elymaic
11000..1107F; Brahmi
11080..110CF; Kaithi
110D0..110FF; Sora Sompeng
11100..1114F; Chakma
11150..1117F; Mahajani
11180..111DF; Sharada
111E0..111FF; Sinhala Archaic Numbers
11200..1124F; Khojki
11280..112AF; Multani
112B0..112FF; Khudawadi
11300..1137F; Grantha
11400..1147F; Newa
11480..114DF; Tirhuta
11580..115FF; Siddham
11600..1165F; Modi
11660..1167F; Mongolian Supplement
11680..116CF; Takri
11700..1174F; Ahom
11800..1184F; Dogra
118A0..118FF; Warang Citi
11900..1195F; Dives Akuru
119A0..119FF; Nandinagari
11A00..11A4F; Zanabazar Square
11A50..11AAF; Soyombo
11AB0..11ABF; Unified Canadian Aboriginal Syllabics Extended-A
11AC0..11AFF; Pau Cin Hau
11C00..11C6F; Bhaiksuki
11C70..11CBF; Marchen
11D00..11D5F; Masaram Gondi
11D60..11DAF; Gunjala Gondi
11EE0..11EFF; Makasar
11FB0..11FBF; Lisu Supplement
11FC0..11FFF; Tamil Supplement
12000..123FF; Cuneiform
12400..1247F; Cuneiform Numbers and Punctuation
12480..1254F; Early Dynastic Cuneiform
12F90..12FFF; Cypro-Minoan
13000..1342F; Egyptian Hieroglyphs
13430..1343F; Egyptian Hieroglyph Format Controls
14400..1467F; Anatolian Hieroglyphs
16800..16A3F; Bamum Supplement
16A40..16A6F; Mro
16A70..16ACF; Tangsa
16AD0..16AFF; Bassa Vah
16B00..16B8F; Pahawh Hmong
16E40..16E9F; Medefaidrin
16F00..16F9F; Miao
16FE0..16FFF; Ideographic Symbols and Punctuation
17000..187FF; Tangut
18800..18AFF; Tangut Components
18B00..18CFF; Khitan Small Script
18D00..18D7F; Tangut Supplement
1AFF0..1AFFF; Kana Extended-B
1B000..1B0FF; Kana Supplement
1B100..1B12F; Kana Extended-A
1B130..1B16F; Small Kana Extension
1B170..1B2FF; Nushu
1BC00..1BC9F; Duployan
1BCA0..1BCAF; Shorthand Format Controls
1CF00..1CFCF; Znamenny Musical Notation
1D000..1D0FF; Byzantine Musical Symbols
1D100..1D1FF; Musical Symbols
1D200..1D24F; Ancient Greek Musical Notation
1D2E0..1D2FF; Mayan Numerals
1D300..1D35F; Tai Xuan Jing Symbols
1D360..1D37F; Counting Rod Numerals
1D400..1D7FF; Mathematical Alphanumeric Symbols
1D800..1DAAF; Sutton SignWriting
1DF00..1DFFF; Latin Extended-G
1E000..1E02F; Glagolitic Supplement
1E100..1E14F; Nyiakeng Puachue Hmong
1E290..1E2BF; Toto
1E2C0..1E2FF; Wancho
1E7E0..1E7FF; Ethiopic Extended-B
1E800..1E8DF; Mende Kikakui
1E900..1E95F; Adlam
1EC70..1ECBF; Indic Siyaq Numbers
1ED00..1ED4F; Ottoman Siyaq Numbers
1EE00..1EEFF; Arabic Mathematical Alphabetic Symbols
1F000..1F02F; Mahjong Tiles
1F030..1F09F; Domino Tiles
1F0A0..1F0FF; Playing Cards
1F100..1F1FF; Enclosed Alphanumeric Supplement
1F200..1F2FF; Enclosed Ideographic Supplement
1F300..1F5FF; Miscellaneous Symbols and Pictographs
1F600..1F64F; Emoticons
1F650..1F67F; Ornamental Dingbats
1F680..1F6FF; Transport and Map Symbols
1F700..1F77F; Alchemical Symbols
1F780..1F7FF; Geometric Shapes Extended
1F800..1F8FF; Supplemental Arrows-C
1F900..1F9FF; Supplemental Symbols and Pictographs
1FA00..1FA6F; Chess Symbols
1FA70..1FAFF; Symbols and Pictographs Extended-A
1FB00..1FBFF; Symbols for Legacy Computing
20000..2A6DF; CJK Unified Ideographs Extension B
2A700..2B73F; CJK Unified Ideographs Extension C
2B740..2B81F; CJK Unified Ideographs Extension D
2B820..2CEAF; CJK Unified Ideographs Extension E
2CEB0..2EBEF; CJK Unified Ideographs Extension F
2F800..2FA1F; CJK Compatibility Ideographs Supplement
30000..3134F; CJK Unified Ideographs Extension G
E0000..E007F; Tags
E0100..E01EF; Variation Selectors Supplement
F0000..FFFFF; Supplementary Private Use Area-A
100000..10FFFF; Supplementary Private Use Area-B
"""
    for line in blockstxt.splitlines():
        try:
            (r, block) = line.split('; ')
            (start, end) = r.split('..')
            start = int(start, 16)
            end = int(end, 16)
            if (start <= x) and (x <= end):
                return block

        except ValueError: continue

    # If we get here, then no block matched.
    return None

def compileit(s):
    "Compile the regular expresion in s, or die trying."
    global ucd, args

    if debug:
        debugprint(s)
        m=search(s,ucd)
        if not m: debugprint("Uncompiled, definitely not in ucd")

    try:
        s=compile(s)
    except error as e:
        err("Error parsing regex: '%s'" % ".*".join(args.characters))
        err(e)
        exit(3)


def isprint(c):
    # Given a category 'c', return True if it is printable.
    # We presume the only non-printable category is 'C' (control).
    # However, technically, much of category 'Z' (spaces) is non-printable.

    # Side note: Python is silly and regex clauses that match an empty
    # string are set to None instead of ''. That's why we doublecheck
    # that category is not None.
    return c and not c.startswith('C')

def iscombining(c):
    # Given a category 'c', return True if it is a combining character.
    # We presume all combining characters are in category 'M' and vice-versa.
    if debug:
        debugprint ("Category is <" + str(c) + ">")  

    return c and c.startswith('M')

def printmatches(s):
    category=""
    foundone=None
    for m in finditer(s, ucd):
        if m:
            foundone=True
            print("   ", end='') 	# Indent for MacOS
            g=m.groupdict()
            debugprint(g)
            if g["hex"] == None: g["hex"]=g["hextwo"]
            if g["name"] == None: g["name"]=g["desctwo"]
            category = g["category"] if g["category"] else g["categorytwo"]
            if g["alias"] == None: g["alias"]=g["aliastwo"]
            c=chr(int(g["hex"], 16))
            if not isprint(category): c="\ufffd" # "Replacement Character"
            if iscombining(category): print('\u25cc', end='') # "Dotted circle"
            print(c, end='\t')
            print ("U+" + g["hex"], end='\t')
            print(g["name"], end='')
            if (len(g["alias"]) and isdifferent(g["name"], g["alias"])):
                print (" (" + g["alias"].lower() + ")", end='')
            print("", flush=True)
            # If -l, show list of installed fonts which contain the glyph.
            if args.list_fonts:  show_fonts( g["hex"], category )

    # Return True if a match was found
    return foundone

def show_fonts(x, category=None):
    """Given a hexadecimal string representing a Unicode code point, 
       list every installed font that defines a glyph at that code point."""
    
    # Normalize whatever they put in. 'U+1D0DE' to 1D0DE
    x=x.lstrip("Uu+0Xx").upper().zfill(4)

    try: fonts = getoutput("fc-list -f '%{family[0]}\t%{style[0]}\t%{file}\n' :charset=" + x + " | sort -u")
    except FileNotFoundError: print(""); return 	# No FontConfig
    except KeyboardInterrupt: exit(1) 			# Exit on ^C

    
    #   debugprint(fonts)		# List of every matching font.
    
    oldfam=""
    for f in fonts.splitlines():
        try: (family, style, filename) = f.split('\t')
        except: continue

        # Don't print the style if it is just "Regular"
        if style == "Regular" or style == "Roman":
            style = ""

        # If we're not doing a long listing, then skip variant styles
        if not args.list_fonts_long:
            if style != "": continue

        # Don't print same font twice (e.g., if installed in more than one
        # location or if there are multiple point sizes of a bitmap font).
        famstr="%(family)s %(style)s\r" % locals()
        if (famstr == oldfam): continue
        oldfam=famstr; oldstyle=style
        
        # Text positioning is hairy due to variable text and font size
        if args.fonts_scale <2: 	# The usual case:
            double_text=False
        else:                           # Alternately, with double size text: 
            double_text=True

        # Some terminals cannot handle double-size text
        if args.never_double_text or not isatty(1):
            double_text=False

        if not double_text:
            famstr = '\t\t' + famstr	# 	Two tabs, sixteen spaces
        else:
            famstr = '\t' + famstr      # 	One double-size tab

        if isatty(1):
            # Writing to a screen? Truncate text if it would wrap.
            (rows, cols) = get_rows_cols()
            if double_text: cols = int(cols/2)
            if len(famstr.expandtabs()) > cols:
                famstr = famstr.expandtabs()[:cols]

        if sixel_capable(): 
            # If terminal can show sixel graphics, do the following:
            # 1. If fonts_scale >=2, double the size of the text of the fontname
            # 2. Position the font name based on the fontscale
            # 3. Render a sixel image of the glyph in the font

            esc=chr(0x1b)
            previous_line = esc + '[A' 	# XXX ought to use terminfo CUU

            # Escape sequence to print double size text (DECDHL)
            if double_text:
                famstr = esc + '#3' + famstr + '\n' \
                    + esc + '#4' + famstr + previous_line

            # Align text lower for 4x, 8x sized font glyphs
            if double_text and args.fonts_scale >= 4:
                famstr = '\n' + famstr + previous_line
            if double_text and args.fonts_scale >=8:
                famstr = '\n' + famstr + previous_line

        print(famstr, end='')

        if not sixel_capable():
            print("")           # stdout is not a tty, so  send a newline

	# If stdout is a terminal, show the glyph as sixels
        if sixel_capable():
            (background, foreground) = get_term_colors()
            (width, height) = get_cell_size()   # Character cell font size
            height = args.fonts_scale * height  # Increase size of font (maybe).
            if not double_text:
                width=(16-1)*width 	# Leave one space before fontname
            else:
                width=(16-2)*width  	# Leave one double-wide space
            debugprint("Font scaling factor: %g" % (args.fonts_scale))
            debugprint("Font canvas size: %g x %g" % (width, height))

            codepoint=chr(int(x,16))

            # If it's a combining character, add a dotted circle to combine with
            if iscombining(category): codepoint='\u25cc'+codepoint

            command=[ "convert",
                      "-size",
                      "%(width)sx%(height)s" % locals(),
                      "xc:" + "%(background)s" % locals(),
                      "-fill", "%(foreground)s" % locals(),
                      "(", 
                      "-background", "none",
                      "-font", "%(filename)s" % locals(),
                      "label:" + codepoint,
                      "-trim",
                      ")",
                      "-gravity", "east",
                      "-compose", "over",
                      "-composite",
                      "+dither",  
                      "-colors", "4", 	# Faster sixels by reducing colors
                      "sixel:-" ]
            debugprint(command)

            try: output=run(command, capture_output=True)
            except FileNotFoundError: print(""); continue 	# No ImageMagick
            except KeyboardInterrupt: exit(1)                   # Exit on ^C
            except ValueError: print(""); continue          # Can't handle NULL

            if output.returncode != 0:
                # Oops, font did not render.
                if double_text: print("")
                print("", flush=True)
                continue

            # Workaround ImageMagick bug by deleting graphics newline (-) at end
            output.stdout.replace(b'-\x1b\\', b'\x1b\\') 

            print("", flush=True, end='')   # Send any pending output to stdout

            try: write(1, output.stdout) 	# Print raw bytes
            except KeyboardInterrupt:		# Catch ^C
                exit(1)                         # Sixels stopped by cleanup()

def sixel_capable():
    # Return true if the terminal can show sixel graphics

    # If stdin or stdout is not a terminal, don't send sixels
    if not args.pipe_sixels and ( not isatty(0) or not isatty(1) ):
        return False

    # XXXX TODO: check DA to see if "4" (sixels) are supported

    return True

def get_rows_cols():
    # Return number of (rows, columns) in current terminal window
    # First, try termios. It's simple but doesn't work everywhere.
    import array, fcntl, termios
    buf = array.array('H', [0, 0, 0, 0])
    if isatty(0):
        fcntl.ioctl(0, termios.TIOCGWINSZ, buf)
    (rows, cols, x, y) = buf;
    if rows==0: rows=24
    if cols==0: cols=80
    return (rows,cols)

def get_cell_size():
    # Inquire from the terminal what its character cell size is.
    # Returns a tuple (width, height) in pixels. 

    # First, try termios. It's simple but doesn't work everywhere.
    import array, fcntl, termios
    buf = array.array('H', [0, 0, 0, 0])
    if isatty(0):
        fcntl.ioctl(0, termios.TIOCGWINSZ, buf)

    # Beware of the stupid output order: y x x y.   --Egmont Koblinger
    (rows, cols, x, y) = buf;
    if rows==0: rows=24
    if cols==0: cols=80

    # VT340 on serial line has x and y set to zero.
    if x==0: x=800
    if y==0: y=480

    from math import floor

    debugprint("rows: %(rows)s\tcols: %(cols)s\tx: %(x)s\ty: %(y)s" % locals())
    debugprint("charcell w: %d\th: %d" % (floor(x/cols), floor(y/rows) ))
    return( floor(x/cols), floor(y/rows) )

def get_term_colors():
    # XXX not implemented yet
    # Inquire from the terminal what its character cell size is.
    # Returns a tuple (background, foreground) in pixels. 
    # fg = read -r -p $'\033]10;?\007' -d$'\007'
    # bg = read -r -p $'\033]11;?\007' -d$'\007'
    return ("gray10","gray90")

def print_double_text(famstr):
    esc=chr(0x1b)
    (rows, cols) = get_rows_cols()
    if len(famstr.expandtabs()) > int(cols/2):
        famstr = famstr.expandtabs()[:int(cols/2)]
    print(esc + '#3' + famstr)         # Top half double-height & width
    print(esc + '#4' + famstr, end='') # Bottom half
    print('\r' + esc + '[A', end='');  # Up 1 line for drawing sixels

    # Note: Should use terminfo for this since VT100 and VT340 differ.

def isdifferent(a, b):
    """Given two strings A & B, determine if B is different enough from A
       that we don't need to print it as well. "Enough" means "adds
       significant new information". For example, the following would
       return False:
    
       "LATIN CAPITAL LETTER A WITH GRAVE", "LATIN CAPITAL LETTER A GRAVE"

       Note that order matters. For example,

       "BROKEN BAR", "BROKEN VERTICAL BAR" 	Should return True
       "BROKEN VERTICAL BAR", "BROKEN BAR" 	Should return False
    """
    a=a.upper()
    b=b.upper()

    if a == b:
        return False
    if a == b.replace(" DIGIT ", " "):
        return False
    if a == b.replace("FORMS ", "BOX DRAWINGS "):
        return False
    if a == b.replace("GRAPHIC ", "SYMBOL "):
        return False
    if a == b.replace("GLYPH ", "PRESENTATION FORM "):
        return False
    if a == b.replace("CENTER", "CENTRE"):
        return False
    if a == b.replace("CENTERED", "CENTRED"):
        return False
    if a == b.replace("SQUARED ", "SQUARE "):
        return False

    b=b.replace(" ", ".*")
    try:
        m=search(b, a)
        if m:
            return False
        else:
            return True
    except:
        # regex should never fail, but if it does, we don't care.
        return True


    # if a.replace("WARDS ", " ") == b:
    #     return False
    # if a.replace("S ", " ") == b:
    #     return False
    # if a.rstrip("S") == b:
    #     return False
    
    return True

def loaducd():
    "Find the UnicodeData.txt file and load it up into ucd global variable."

    global ucd
    ucd=None
    ucdplaces=( "/usr/local/share/unicode/UnicodeData.txt",
                "/usr/share/unicode/UnicodeData.txt",
                "~/.local/share/unicode/UnicodeData.txt",
                "UnicodeData.txt" )
    for f in ucdplaces:
        f=expanduser(expandvars(f)) 		# Python's open() is silly
        try:
            ucd=open(f).read()
            debugprint("Found data file at " + f)
        except:
            continue

    # Sanity check: did we find the UnicodeData.txt file?
    if ucd == None:
        eprint("""\
    Could not find UnicodeData.txt in:

%s
    On most GNU/Linux systems your package manager can install it easily.
    For example:  apt install unicode-data.

    Alternately, you can grab it via wget like so:

        mkdir -p ~/.local/share/unicode
	cd ~/.local/share/unicode
        wget ftp://ftp.unicode.org/Public/UNIDATA/UnicodeData.txt
""" %
               ("".join(["    " + s + "\n" for s in ucdplaces])))
        exit(2)

    if debug:
        debugprint("Number of Unicode chars: %d" % len(ucd.splitlines()))

    return ucd


def makeregexcharname(words):
    "Create a regular expression to search for a character name"

    # Format of ucd: each character is on a separate line.
    # Each line is fifteen columns separated by semicolons.
    # We only care about columns 1, 2, 3 and 11
    #      1: Code value
    #      2: Character name
    #      3: General category
    #     11: ISO 10646 comment field (usually an alias)
    #
    # For example:
    #     002F;SOLIDUS;Po;0;CS;;;;;N;SLASH;;;;
    #
    # See Tech Report 44 for more details.
    #
    # XXX Todo: maybe look up in table of confusable entities.

    # combine and quote the command line arguments so we can use them in
    # verbose regex. Also, allow the user to use ^ and $, just like in awk
    # to refer to the beginning and end of the field instead of line.
    words=["("+x+")" for x in words]       # wrap in parens to fix alternation
    arghs=".*".join(words)                 # search terms can have junk between
    arghs=sub(r"(\s)", r"\\\1", arghs)     # quote whitespace for verbose regex
    arghs=sub(r"^\(\^", r"(?<=;)(", arghs) # ^ matches semicolon before field 
    arghs=sub(r"\$\)$", r"(?=;))", arghs)  # $ matches semicolon after field 

    # s is a search regex for field 2 (name) based on the command line arguments
    s=r"""^
        (?P<hex>[^;]*);             # first field is hexadecimal codepoint
        (?P<name>[^;]*"""+arghs+"""[^;]*); # field 2 (charactername) matches?
        (?P<category>[^;]*);        # third field is general category 
        ([^;]*;){7}                 # skip next 7 fields
        (?P<alias>[^;]*)            # field 11 is comment/alias
        (;[^;]*){4}$                # line ends with four more fields
    """

    # r is a search regex for field 11 (alias)
    r=r"""^
        (?P<hextwo>[^;]*);            # first field is hexadecimal codepoint
        (?P<desctwo>[^;]*);           # second field is charactername
        (?P<categorytwo>[^;]*);       # third field is general category 
        ([^;]*;){7}                   # skip next 7 fields
        (?P<aliastwo>[^;]*("""+arghs+""")[^;]*) # field 11 (alias) matches?
        (;[^;]*){4}$                  # line ends with four more fields
    """

    s=s+"|"+r                   # Search for either name or alias
    s="(?mix)"+s                # multiline, case insensitive, verbose

    debugprint("Complete search regex is " + s)

    return s

def makeregexcodepoint(cp):
    "Make a regular expression to search for a hexadecimal codepoint"

    # User specified a codepoint, e.g., "U+23fb" 
    # Normalize whatever they put in into "23FB"
    cp=cp.lstrip("Uu+0Xx").upper().zfill(4)

    # s is a search regex for field 1 (hexadecimal)
    s=r"""^
        (?P<hex>"""+cp+""");		#first field is hex codepoint
        (?P<name>[^;]*);		# second field is charactername
        (?P<category>[^;]*);		# third field is category
        ([^;]*;){7}			# skip next 7 fields
        (?P<alias>[^;]*)            	# field 11 is comment/alias
    """
    s="(?mix)"+s                     # multiline, case insensitive, verbose
    return(s)



def rangeexpansion(oldargv):
    """Look for ranges "a..z" in oldargv[] and expand by inserting new elements.
    The new argv is returned to the caller. 

    0..7 	-->	0 1 2 3 4 5 6 7
    abc0..7 	--> 	abc0 abc1 abc2 abc3 abc4 abc5 abc6 abc7
    23b8..ff	-->	23b8 23b9 23ba 23bb ... 23fd 23fe 23ff

    Notes:

    * Does check for (and correctly ignores) prefix of U+ or 0x.
    * Should check for invalid ranges and ignore them instead of barfing.

    Design notes:

    * Although similar to shell brace expansion, the shell is
      inadequate because it is decimal only.
    """

    s=r"""(?mix)
	  (U\+?|0?x)?
    	  (?P<start>[0-9A-F]+)
	  \.\.                 # Literal two periods. MANDATORY.
	  (U\+?|0?x)?
	  (?P<end>[0-9A-F]+)
    """
    s=compile(s)
    newargv=[]

    for arg in oldargv:
        m=search(s, arg)
        if not m:
            newargv.append(arg)
        else:
            debugprint("Doing range expansion on " + arg)
            g=m.groupdict()
            start=g["start"]
            end=g["end"]

            end=start[:-len(end)]+end
            debugprint("Found range from " + start + " to " + end)
            z=max(len(start), len(end))
            try:
                start=int(start, 16)
                end=int(end, 16)
                if start > end: raise ValueError

                for i in range(start, end+1):
                    hexits=hex(i)[2:].zfill(z)
                    newargv.append( hexits )
                    debugprint("   added: " + hexits)
            except ValueError:
                newargv.append(arg)

    return(newargv)


def eprint(*args, **kwargs):
    "Print to stderr"
    print(*args, file=stderr, **kwargs)

def err(*args, **kwargs):
    "Print to stderr with program name prefixed"
    eprint(basename(argv[0]) + ": ", end='')
    eprint(*args, **kwargs)

def debugprint(*args, **kwargs):
    "If debug var is True, print to stderr with progname prefix"
    if (debug):
        err(*args, **kwargs)

def cleanup():
    if isatty(2):
        termios.tcdrain(2) 		# Wait for stderr to be printed
    if isatty(1):
        termios.tcflush(1, termios.TCOFLUSH) 	# Discard stdout
    esc=chr(0x1b)
    eprint(esc+"\\", end='')		# String Terminator for sixels (ST)
    eprint(esc+"#5", end='')		# Single width line (DECSWL)
    eprint("\r" + esc + "[J", end='')	# Erase to end of display (ED)

######################################################################

import atexit
atexit.register(cleanup)        	# When exiting, cleanup sixels


# VT340 is Latin-1 ISO8859-1 encoding.
# Python3 defaults to dying horribly on simple things like print('\u2020')
import sys
if sys.stdout:
    sys.stdout.reconfigure(errors='replace') 	# Print a ? instead of dying.


### Run the main routine
try:
    main()
except BrokenPipeError:
    # Ignore non-error errors. For example: 'ugrep -w pi | head'
    sys.stdout = None;
    cleanup()
except KeyboardInterrupt:		# Catch ^C
    cleanup()

atexit.unregister(cleanup)      # Normal exit, no need to cleanup.

debugprint("Normal exit")

# Implementation notes:

# This is a rewrite of b9's AWK ugrep in Python. While AWK makes a lot
# more sense for what this program does (comparing fields based on
# regexps), a rewrite was necessary because GNU awk, while plenty
# powerful, uses \y for word edges instead of \b and that was bugging
# me. Gawk does this for backwards compatibility with historic AWK,
# which is all well and good, but gawk has no way to disable it for
# new scripts.
#
# Switching to Python did have the benefit of allowing more powerful
# Perl-like regexes (not that anyone has requested that).
# 
# One downside is that I needed a huge hairy regex to simply search
# only in a certain field of each line. Maybe there's some Pythonic
# way to do it, but it's not obvious. Perhaps a 2D array?
#
# Also, I took for granted that awk let me use ^ and $ to search for
# the beginning and ending of fields instead of lines. I tried to
# reimplement that in Python, but it's not quite right as it only
# checks the first and last character. For example, ugrep "^x" works,
# but ugrep "(^x)" does not.


# Note: I do not use Python's `unicodedata` module because it is
# insufficient. It allows one to search by character name only if the
# precise name: `unicodedata.lookup("ROTATED HEAVY BLACK HEART BULLET")`.

