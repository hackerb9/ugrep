#!/usr/bin/python3
# ugrep: find unicode characters based on their names or number.
# 	 Essentially grep for the Unicode table.

# PREREQUISITE: 
#
# * You'll need a copy of UnicodeData.txt installed. 
#   On Debian GNU/Linux, this can be done by `apt install unicode-data`.
#   Or, you can download it by hand from the Unicode Consortium and place
#   it in `~/.local/share/unicode/UnicodeData.txt`.

# ADDED FUNCTIONALITY: 
#
# * If you have FontConfig installed, then you can use -l to see which
#   fonts contain a certain character.
#
# * If you have ImageMagick installed and your terminal can display
#   sixels, then -l will also show a rendering of the character in
#   each font.
#
# * If you have Unihan_Readings.txt then CJK ideographs will show definitions.
#


# Fun things to try:

# ugrep alchemical 
# ugrep ornament
# ugrep bullet
# ugrep 'vine|bud'
# ugrep vai
# ugrep heavy
# ugrep drawing
# ugrep 2300..ff

# GPL â‰¥3 (see LICENSE file)
# B9 September 2018 â€“ October 2021

debug=True
debug=False

from re import *
from sys import argv, stderr
from os.path import expanduser, expandvars, basename
from os import isatty
from subprocess import getoutput, run
import argparse
from math import floor, copysign
from collections import defaultdict

from pprint import pprint as pp

try:
    from posix import write
    import select
    import termios 
    import tty
except ImportError:
    print("Sorry, but your system isn't POSIX compatible", file=stderr)
    exit( 1 )

# Allow fancier terminfo formatting, if python3-blessed is installed.
try:
    import blessed
    curses = blessed.terminal.Terminal()
except ImportError:
    pass



def usage():
    print("""\
ugrep: find unicode characters based on their names or codepoints
Usage: 
    ugrep [-wlx] [-L n] <character> | <codepoint ...> | <character name>
    ugrep -c <character string ...>

              -w:  Match only whole words:		ugrep -w pi
              -c:  Show each character in a string:	ugrep -c "(ï¾Ÿâˆ€ï¾Ÿ)"
              -l:  List installed fonts that include matching characters.
             	   -ll: Also includes style variants (bold, italic,...)
        	   -L scale:  Scale font examples from 2 to 8x.
    -x, -x2, -x3:  Show example usage.

       <character> A single character; implies -c:	ugrep â˜™

       <codepoint> One or more hexadecimal numbers:	ugrep U+1F639
		   Ranges are allowed with two dots:	ugrep 23b0..f
	           Optional range increment:		ugrep 0..ffff..1000

  <character name> A Unicode name (as a regex), e.g.:	ugrep alchemical

<character string> One or more characters. e.g.: 	ugrep -c "( Í¡Â° ÍœÊ– Í¡Â°)"

Try 'ugrep --examples' to see typical usage examples.
""", end='' )

def examples():
    print("""\
# You can search by character name.
    $ ugrep heart
    â˜™	U+2619	REVERSED ROTATED FLORAL HEART BULLET
    â£	U+2763	HEAVY HEART EXCLAMATION MARK ORNAMENT
    â¤	U+2764	HEAVY BLACK HEART
    [ ... examples truncated for brevity ... ]
    ðŸ˜»	U+1F63B	SMILING CAT FACE WITH HEART-SHAPED EYES

# Or, you can search by pasting in a specific character.
    $ ugrep âœ¿
    âœ¿       U+273F  BLACK FLORETTE

# Or, you can search by code point.
    $ ugrep 273a
    âœº       U+273A  SIXTEEN POINTED ASTERISK

# By default, words match anywhere.
    $ ugrep clos brac			# Equivalent to "clos.*brac"
    ]       U+005D  RIGHT SQUARE BRACKET (closing square bracket)
    }       U+007D  RIGHT CURLY BRACKET (closing curly bracket)
    ã€‰      U+3009  RIGHT ANGLE BRACKET (closing angle bracket)
"""
    )

def examples2():
    print("""\
# Use -w to match only whole words.
    $ ugrep -w "R"			# Equivalent to "\\bR\\b"
    R	U+0052  LATIN CAPITAL LETTER R
    â„›	U+211B  SCRIPT CAPITAL R (Script r)
    â„œ	U+211C  BLACK-LETTER CAPITAL R (Black-letter r)
    â„	U+211D  DOUBLE-STRUCK CAPITAL R (Double-struck r)

# Use -c to display info for each character in a string.
    $ ugrep -c "á••( á› )á•—"
    á••	U+1555	CANADIAN SYLLABICS FI
    (	U+0028	LEFT PARENTHESIS (opening parenthesis)
    á›	U+141B	CANADIAN SYLLABICS NASKAPI WAA
    )	U+0029	RIGHT PARENTHESIS (closing parenthesis)
    á•—	U+1557	CANADIAN SYLLABICS FO

# Use 'U+' prefix to force a search for hexadecimal
    $ ugrep u+ee			# "0xEE" also works
    Ã®   U+00EE  LATIN SMALL LETTER I WITH CIRCUMFLEX
"""
    )

def examples3():
    print("""\
# Regex ^ and $ work, mostly
    $ ugrep ^x				
    âŠ»	U+22BB	XOR
    âŒ§	U+2327	X IN A RECTANGLE BOX (clear key)

# ugrep finds aliases (and puts them in parens)
    $ ugrep backslash
    \	U+005C	REVERSE SOLIDUS (backslash)

# Use -l to see which fonts are installed that can render a certain character
    $ ugrep -l math left tortoise
    âŸ¬    U+27EC  MATHEMATICAL LEFT WHITE TORTOISE SHELL BRACKET
                 Symbola 
                 Unifont 

# Use -L n to scale the font examples n-times larger; implies -l
    $ ugrep -L2 antimony
    [ Cannot show sixel examples here ]

# Show every single Unicode character.
    $ ugrep '.?'  |  less
"""
    )




# Main
def main():
    global argv, args, debug

    # Load the Unicode Data into the global ucd variable.
    loaducd()
    
    if len(argv) == 1:
        usage()                 # ArgParse's usage is ugly, use our own.
        exit(1)

    if argv[1]=='-h' or argv[1]=='--help':
        usage()
        exit(0)

    if argv[1].startswith('-x') or argv[1].startswith('--example'):
        try: x=int(argv[1][-1])
        except ValueError: x=0
        if x == 0:
            examples()
            if isatty(1):
                print("Try 'ugrep --examples=2' to see more usage examples.")
            else:
                examples2()
                examples3()
        if x == 1:
            examples()
            print("Try 'ugrep --examples=2' to see more usage examples.")
        if x == 2:
            examples2()
            print("Try 'ugrep --examples=3' to see even more usage examples.")
        if x == 3:
            examples3()
            print("Try 'ugrep -x | less' to see all usage examples in one page.")
        exit(0)

    parser = argparse.ArgumentParser(
        description='find Unicode characters by name, number, or example.')
    parser.add_argument(
        'characters', metavar='Character Name | Codepoint | Character', 
        nargs='+', type=str,
        help='which character to show specified by name, codepoint or literal example')
    parser.add_argument('-w', '--word-regexp', action="store_true",
        help='matches only whole words, e.g., ugrep -w pi')
    parser.add_argument('-c', '--char-by-char', action="store_true",
        help='show each character in a string, e.g., ugrep -c "(ï¾Ÿâˆ€ï¾Ÿ)"')
    parser.add_argument('-l', '--list-fonts', action="store_true",
        help='list fonts that include a glyph for a given character')
    parser.add_argument('-ll', '--list-fonts-long', action="store_true",
        help='list all fonts, including variants like italic, bold, etc.')
    # XXX Argh. argparse gives an error on 'ugrep -L foo' but works for 'ugrep foo -L'.
    parser.add_argument('-L', '--list-fonts-scale', metavar='scaling', dest='fonts_scale',
                        nargs='?', const=2, default=None, type=float,
        help='list fonts, scaling the characters to be from 2 to 8x larger')
    parser.add_argument('--never-double-text', action="store_true",
        help="Don't use double-size terminal text even when font scale >= 2")
    parser.add_argument('--pipe-sixels', action="store_true",
        help="Allow sixel escape sequences when redirecting stdout")
    parser.add_argument('-x', '--examples',  action="store_true",
        help='show example usage')
    parser.add_argument('--debug', action="store_true", help='internal debugging')

    args = parser.parse_args()

    # -ll (list_fonts_long) same as -l but shows all font variants
    if args.list_fonts_long:
        args.list_fonts = True

    # -l (list fonts), defaults to a scaling of 1x
    if args.list_fonts and not args.fonts_scale:
        args.fonts_scale = 1

    # if -L is like -l (list fonts), but with larger images.
    if args.fonts_scale:
        args.list_fonts = True;

    if args.debug: debug=True

    # If -w, then wrap each argument with \b (regexp word breaks)
    if args.word_regexp:
        args.characters = [ "\\b"+s+"\\b" for s in args.characters ]

    # Is it a single character? (ugrep 'A') Then only show that one character.
    if  len(args.characters)==1 and len(args.characters[0])==1:
        args.char_by_char = True 	# Pretend -c flag was given.

    # Is it -c followed by a string of characters?
    if args.char_by_char:
        # For each string s, look up each character in the string.
        for s in args.characters:
            for i in range(len(s)):
                for j in range(0,len(s[i])):
                    arg = hex(ord(s[i][j]))
                    if not showcodepoint( arg ):
                        # Not Unicode, but could be Chinese, Japanese, Korean
                        if not showcjk( arg[2:] ):
                            # Not CJK, so print block name
                            showcharacterblock( arg )

            if s is not args.characters[-1]:
                print("")       # Space between words, if multiple args 
        exit(0)

    # Do range expansion on argv. (e.g., 2301..f)
    args.characters = rangeexpansion(args.characters)

    # First, try looking up each arg individually as hexadecimal codepoints.
    # Loop to allow input like "ugrep 23b{0..9}" (NB: slower than 23b[0-9])
    # Must be hexadecimal.
    for arg in args.characters:
        if arg.strip("Uu+0123456789ABCDEFXabcdefx[]-.*?^$(){,}"):
            debugprint("Stopped looking for hex/regex arguments at", arg)
            break
        # Show matches in UnicodeData.txt for each hex codepoint
        if not showcodepoint(arg):
            # Hmm... it wasn't in UnicodeData. Maybe CJK?
            if not showcjk(arg):
                # Nope, print the character anyway and its Unicode block.
                showcharacterblock(arg)
            # If -l, show installed fonts which contain the glyph.
            if args.list_fonts: show_fonts(arg)

    # Also look up the entire argv[] as a character name or alias. 
    # Must NOT begin with U+ or 0x.
    if not match("(?i)^U\+|^0x", args.characters[0]):
        s=makeregexcharname(args.characters)
        if printmatches(s):
            debugprint("argv[] array matched as a whole.")

def showonecharacter(c):
    s=makeregexeither(c)
    if debug: compileit(s)
    return printmatches(s)
              
def showcodepoint(c):
    s=makeregexcodepoint(c)
    if debug: compileit(s)
    return printmatches(s)
              
def showcjk(x):
    """Prints a line for a codepoint x (hexadecimal string) using the data
    from Unihan_Readings.txt. If that file is not available or if the
    codepoint is not in a CJK region, then return False.

    """

    x=x.upper().lstrip("Uu+0Xx")             # "u+face" -> "FACE"
    try:
        codepoint=cp2int(x)
    except ValueError:
        return False
    
    c=chr(codepoint)
    if not isunihan(codepoint):
        return False

    # Get the dictionary of "readings" for character x
    r = lookup_readings(x)
    try:
        definition = r["kDefinition"]
    except TypeError:
        return False
    except KeyError:
        definition = '[' + getblock(cp2int(x)) + ']'

    transliteration=[]
    shortlang = {
        "kMandarin": "M",
        "kCantonese": "C",
        "kJapaneseKun": "J", 
        "kKorean": "K", 
        "kVietnamese": "V" 
    }
    for lang in (shortlang.keys()):
        try:
            transliteration.append(shortlang[lang] + ": " + r[lang] + ",")
        except KeyError:
            pass

    if transliteration:
        transliteration[-1] = transliteration[-1].rstrip(",")

    print("   ", end='') 	# Indent for MacOS
    print(c, end='\t')
    print ("U+" + x, end='\t')
    print(definition, end='')
    if transliteration:
        print(" (", *transliteration, ")", end='')
    print("", flush=True) 

    return True

def isunihan(x):
    """Given a codepoint x, e.g. "U+8000", return True if the x is within
    a CJK Unified Ideographs block. Such characters are not defined in
    UnicodeData.txt but can be looked up in Unihan_Readings.txt.

    """
    try:
        codepoint=cp2int(x)     		# Convert hex str x to integer
    except ValueError:
        return False            		# Could not convert

    if getblock(codepoint).find('Ideographs') != -1:
        return True
    else:
        return False
    
    
def lookup_readings(cp):
    """Given a hexadecimal string, cp ("2BAC3"), return a dictionary with
    an entry for each line in Unihan_Readings.txt. Example:
    
    For input "2BAC3", the result is:
    {'kDefinition':  '(CANT.) SARCASTIC INTERROGATIVE',
     'kCantonese':   'e1' }

    For input "9EBB", note that some of the results include embedded UTF-8:
    { 'kCantonese': 'maa4', 
      'kDefinition': 'hemp, jute, flax; sesame', 
      'kHangul': '\xeb\xa7\x88:0E', 
      'kHanyuPinlu': 'm\xc3\xa1(159) ma(11)', 
      'kHanyuPinyin': '74723.010:m\xc3\xa1,m\xc4\x81', 
      'kJapaneseKun': 'ASA', 
      'kJapaneseOn': 'MA BA', 
      'kKorean': 'MA', 
      'kMandarin': 'm\xc3\xa1', 
      'kTGHZ2013': '240.080:m\xc3\xa1', 
      'kTang': '*ma', 
      'kVietnamese': 'ma', 
      'kXHC1983': '0753.040:m\xc4\x81 0753.110:m\xc3\xa1'
    }
    """
    # As of 2021 Unihan includes these keys, sorted by frequency:
    # cat Unihan_Readings.txt | cut -f2 | sort | uniq -c | sort -n
    ############################################################
    #   3800  kHanyuPinlu   Mandarin Pinyin from XDHYPLCD      #
    #   3812  kTang         Tang dynasty pronunciation         #
    #   8106  kTGHZ2013     Mandarin Pinyin from TGHZ          #
    #   8301  kVietnamese   Vietnamese transliteration         #
    #   8414  kHangul       Korean pronunciation in Hangul     #
    #   9051  kKorean       Korean in Yale romanization        #
    # 11,018  kXHC1983      Mandarin Pinyin from XHC           #
    # 11,293  kJapaneseKun  Japanese Hepburn romanization      #
    # 13,176  kJapaneseOn   Sino-Japanese pronunciation        #
    # 21,076  kDefinition   A short gloss in English           #
    # 23,112  kCantonese    Cantonese Jyutping transliteration #
    # 34,131  kHanyuPinyin  Mandarin Pinyin from HDZ           #
    # 41,378  kMandarin     Mandarin Pinyin (most customary)   #
    ############################################################
    # For more details, see Unicode Standard Annex #38 
    # https://www.unicode.org/reports/tr38/tr38-31.html

    # Initialize 'readings' global variable
    if not loadreadings():
        return False

    cp = cp.lstrip("Uu+0Xx")
    results={}
    # Example entry from Unihan_Readings.txt:
    # U+2BAC3 kCantonese      e1
    # U+2BAC3 kDefinition     (Cant.) sarcastic interrogative
    # Regex to match a line in Unihan_Readings.txt
    r=r"\nU\+" + cp + "\t+" + "(?P<key>[^\t]+)" + "\t+" + "(?P<value>[^\t\n]+)"
    if type(readings) is bytes: r=bytes(r, 'utf-8')
    matches=findall(r, readings)
    if matches == None: 
        debugprint("lookup_readings did not find", r)
        return None
    debugprint("lookup_readings found ", matches)
    for m in matches:
        if type(m[0]) is bytes:		 # bz lib's read() returns 'bytes'
            m0=str(m[0], 'UTF-8')
            m1=str(m[1], 'UTF-8')
        else:
            m0=m[0]             	# normal read() returns 'str'
            m1=m[1]
        results[m0] = m1        # Build dictionary. 
    return results

def lookup_namealias(cp):
    """Given a hexadecimal string, cp ("0004"), return a list with
    an element for each line in NameAliases.txt. Example:
    For input "0004", the result is: { 'end of transmission', 'EOT' }
    """
    # Initialize 'namealiases' global variable
    if not loadnamealiases():
        return False

    cp = cp.lstrip("Uu+0Xx")
    results=[]
    # Example entry from NameAliases.txt:
    # 0004;END OF TRANSMISSION;control
    # 0004;EOT;abbreviation
    # 
    # Regex to match a line in NameAliases.txt
    r=r"(?m)^%04X;(?P<alias>[^;\n]+);(?P<type>[^;\n]+)$" % (cp2int(cp))
    matches=finditer(r, namealiases)
    if matches == None: 
        debugprint("lookup_namealias did not find", r)
        return None

    for m in matches:
        g=m.groupdict()
        debugprint("lookup_namealias", g)
        if g['type'] != 'abbreviation':
            # For clarity, usually only names are in all caps.
            results.append(g['alias'].lower())  # "end of transmission"
        else:
            # Allow abbreviations to also be uppercase 
            results.append(g['alias']) 		# "EOT"


    return results

def showcharacterblock(codepoint):

    """Prints a line showing which character block a hex number belongs in.
    If showonecharacter() fails, this can be used to at least print something,
    """
    try:
        x=cp2int(codepoint)         # Remove U+ or 0x prefix and return integer
    except ValueError:
        return

    block=getblock(x)
    if block:
        try: c=chr(x)
        except ValueError: return
        print("   ", end='') 	# Indent for MacOS
        print(c, end='')
        print('\t' + int2cp(x) + '\t', end='')
        print('Block: [' + block + ']')
        
    
def cp2int(cp):
    """Given a Unicode codepoint as a string (e.g., U+1D0DE)
    convert the hexadecimal number to an integer and return it.
    Throws ValueError if cp is not convertible to an integer.
    """
    if type(cp) is int: return cp
    if cp is None: return 0
    cp = cp.lstrip("Uu+0Xx")
    if cp == '': return 0
    return( int(cp.lstrip("Uu+0Xx"), 16) )

def int2cp(x):
    "Given an int, return it as a hexadecimal string in the format U+1D0DE)."
    return( "U+%04X" % (x) )

def getblock(x):
    """Given an integer representing a Unicode code point, return its
    block. E.g, U+0041 -> Basic Latin, U+1F675 -> Ornamental Dingbats.
    This is useful to identify characters that are not officially part
    of Unicode, for example all the CJK Ideographs from 4E00 to 9FFF.

    An empty string is returned if the integer is not within any
    Unicode block, e.g., ugrep U+80000.

    """

    # Data from Unicode 2021 Blocks-14.0.0.txt
    blockstxt="""
0000..007F; Basic Latin
0080..00FF; Latin-1 Supplement
0100..017F; Latin Extended-A
0180..024F; Latin Extended-B
0250..02AF; IPA Extensions
02B0..02FF; Spacing Modifier Letters
0300..036F; Combining Diacritical Marks
0370..03FF; Greek and Coptic
0400..04FF; Cyrillic
0500..052F; Cyrillic Supplement
0530..058F; Armenian
0590..05FF; Hebrew
0600..06FF; Arabic
0700..074F; Syriac
0750..077F; Arabic Supplement
0780..07BF; Thaana
07C0..07FF; NKo
0800..083F; Samaritan
0840..085F; Mandaic
0860..086F; Syriac Supplement
0870..089F; Arabic Extended-B
08A0..08FF; Arabic Extended-A
0900..097F; Devanagari
0980..09FF; Bengali
0A00..0A7F; Gurmukhi
0A80..0AFF; Gujarati
0B00..0B7F; Oriya
0B80..0BFF; Tamil
0C00..0C7F; Telugu
0C80..0CFF; Kannada
0D00..0D7F; Malayalam
0D80..0DFF; Sinhala
0E00..0E7F; Thai
0E80..0EFF; Lao
0F00..0FFF; Tibetan
1000..109F; Myanmar
10A0..10FF; Georgian
1100..11FF; Hangul Jamo
1200..137F; Ethiopic
1380..139F; Ethiopic Supplement
13A0..13FF; Cherokee
1400..167F; Unified Canadian Aboriginal Syllabics
1680..169F; Ogham
16A0..16FF; Runic
1700..171F; Tagalog
1720..173F; Hanunoo
1740..175F; Buhid
1760..177F; Tagbanwa
1780..17FF; Khmer
1800..18AF; Mongolian
18B0..18FF; Unified Canadian Aboriginal Syllabics Extended
1900..194F; Limbu
1950..197F; Tai Le
1980..19DF; New Tai Lue
19E0..19FF; Khmer Symbols
1A00..1A1F; Buginese
1A20..1AAF; Tai Tham
1AB0..1AFF; Combining Diacritical Marks Extended
1B00..1B7F; Balinese
1B80..1BBF; Sundanese
1BC0..1BFF; Batak
1C00..1C4F; Lepcha
1C50..1C7F; Ol Chiki
1C80..1C8F; Cyrillic Extended-C
1C90..1CBF; Georgian Extended
1CC0..1CCF; Sundanese Supplement
1CD0..1CFF; Vedic Extensions
1D00..1D7F; Phonetic Extensions
1D80..1DBF; Phonetic Extensions Supplement
1DC0..1DFF; Combining Diacritical Marks Supplement
1E00..1EFF; Latin Extended Additional
1F00..1FFF; Greek Extended
2000..206F; General Punctuation
2070..209F; Superscripts and Subscripts
20A0..20CF; Currency Symbols
20D0..20FF; Combining Diacritical Marks for Symbols
2100..214F; Letterlike Symbols
2150..218F; Number Forms
2190..21FF; Arrows
2200..22FF; Mathematical Operators
2300..23FF; Miscellaneous Technical
2400..243F; Control Pictures
2440..245F; Optical Character Recognition
2460..24FF; Enclosed Alphanumerics
2500..257F; Box Drawing
2580..259F; Block Elements
25A0..25FF; Geometric Shapes
2600..26FF; Miscellaneous Symbols
2700..27BF; Dingbats
27C0..27EF; Miscellaneous Mathematical Symbols-A
27F0..27FF; Supplemental Arrows-A
2800..28FF; Braille Patterns
2900..297F; Supplemental Arrows-B
2980..29FF; Miscellaneous Mathematical Symbols-B
2A00..2AFF; Supplemental Mathematical Operators
2B00..2BFF; Miscellaneous Symbols and Arrows
2C00..2C5F; Glagolitic
2C60..2C7F; Latin Extended-C
2C80..2CFF; Coptic
2D00..2D2F; Georgian Supplement
2D30..2D7F; Tifinagh
2D80..2DDF; Ethiopic Extended
2DE0..2DFF; Cyrillic Extended-A
2E00..2E7F; Supplemental Punctuation
2E80..2EFF; CJK Radicals Supplement
2F00..2FDF; Kangxi Radicals
2FF0..2FFF; Ideographic Description Characters
3000..303F; CJK Symbols and Punctuation
3040..309F; Hiragana
30A0..30FF; Katakana
3100..312F; Bopomofo
3130..318F; Hangul Compatibility Jamo
3190..319F; Kanbun
31A0..31BF; Bopomofo Extended
31C0..31EF; CJK Strokes
31F0..31FF; Katakana Phonetic Extensions
3200..32FF; Enclosed CJK Letters and Months
3300..33FF; CJK Compatibility
3400..4DBF; CJK Unified Ideographs Extension A
4DC0..4DFF; Yijing Hexagram Symbols
4E00..9FFF; CJK Unified Ideographs
A000..A48F; Yi Syllables
A490..A4CF; Yi Radicals
A4D0..A4FF; Lisu
A500..A63F; Vai
A640..A69F; Cyrillic Extended-B
A6A0..A6FF; Bamum
A700..A71F; Modifier Tone Letters
A720..A7FF; Latin Extended-D
A800..A82F; Syloti Nagri
A830..A83F; Common Indic Number Forms
A840..A87F; Phags-pa
A880..A8DF; Saurashtra
A8E0..A8FF; Devanagari Extended
A900..A92F; Kayah Li
A930..A95F; Rejang
A960..A97F; Hangul Jamo Extended-A
A980..A9DF; Javanese
A9E0..A9FF; Myanmar Extended-B
AA00..AA5F; Cham
AA60..AA7F; Myanmar Extended-A
AA80..AADF; Tai Viet
AAE0..AAFF; Meetei Mayek Extensions
AB00..AB2F; Ethiopic Extended-A
AB30..AB6F; Latin Extended-E
AB70..ABBF; Cherokee Supplement
ABC0..ABFF; Meetei Mayek
AC00..D7AF; Hangul Syllables
D7B0..D7FF; Hangul Jamo Extended-B
D800..DB7F; High Surrogates
DB80..DBFF; High Private Use Surrogates
DC00..DFFF; Low Surrogates
E000..F8FF; Private Use Area
F900..FAFF; CJK Compatibility Ideographs
FB00..FB4F; Alphabetic Presentation Forms
FB50..FDFF; Arabic Presentation Forms-A
FE00..FE0F; Variation Selectors
FE10..FE1F; Vertical Forms
FE20..FE2F; Combining Half Marks
FE30..FE4F; CJK Compatibility Forms
FE50..FE6F; Small Form Variants
FE70..FEFF; Arabic Presentation Forms-B
FF00..FFEF; Halfwidth and Fullwidth Forms
FFF0..FFFF; Specials
10000..1007F; Linear B Syllabary
10080..100FF; Linear B Ideograms
10100..1013F; Aegean Numbers
10140..1018F; Ancient Greek Numbers
10190..101CF; Ancient Symbols
101D0..101FF; Phaistos Disc
10280..1029F; Lycian
102A0..102DF; Carian
102E0..102FF; Coptic Epact Numbers
10300..1032F; Old Italic
10330..1034F; Gothic
10350..1037F; Old Permic
10380..1039F; Ugaritic
103A0..103DF; Old Persian
10400..1044F; Deseret
10450..1047F; Shavian
10480..104AF; Osmanya
104B0..104FF; Osage
10500..1052F; Elbasan
10530..1056F; Caucasian Albanian
10570..105BF; Vithkuqi
10600..1077F; Linear A
10780..107BF; Latin Extended-F
10800..1083F; Cypriot Syllabary
10840..1085F; Imperial Aramaic
10860..1087F; Palmyrene
10880..108AF; Nabataean
108E0..108FF; Hatran
10900..1091F; Phoenician
10920..1093F; Lydian
10980..1099F; Meroitic Hieroglyphs
109A0..109FF; Meroitic Cursive
10A00..10A5F; Kharoshthi
10A60..10A7F; Old South Arabian
10A80..10A9F; Old North Arabian
10AC0..10AFF; Manichaean
10B00..10B3F; Avestan
10B40..10B5F; Inscriptional Parthian
10B60..10B7F; Inscriptional Pahlavi
10B80..10BAF; Psalter Pahlavi
10C00..10C4F; Old Turkic
10C80..10CFF; Old Hungarian
10D00..10D3F; Hanifi Rohingya
10E60..10E7F; Rumi Numeral Symbols
10E80..10EBF; Yezidi
10F00..10F2F; Old Sogdian
10F30..10F6F; Sogdian
10F70..10FAF; Old Uyghur
10FB0..10FDF; Chorasmian
10FE0..10FFF; Elymaic
11000..1107F; Brahmi
11080..110CF; Kaithi
110D0..110FF; Sora Sompeng
11100..1114F; Chakma
11150..1117F; Mahajani
11180..111DF; Sharada
111E0..111FF; Sinhala Archaic Numbers
11200..1124F; Khojki
11280..112AF; Multani
112B0..112FF; Khudawadi
11300..1137F; Grantha
11400..1147F; Newa
11480..114DF; Tirhuta
11580..115FF; Siddham
11600..1165F; Modi
11660..1167F; Mongolian Supplement
11680..116CF; Takri
11700..1174F; Ahom
11800..1184F; Dogra
118A0..118FF; Warang Citi
11900..1195F; Dives Akuru
119A0..119FF; Nandinagari
11A00..11A4F; Zanabazar Square
11A50..11AAF; Soyombo
11AB0..11ABF; Unified Canadian Aboriginal Syllabics Extended-A
11AC0..11AFF; Pau Cin Hau
11C00..11C6F; Bhaiksuki
11C70..11CBF; Marchen
11D00..11D5F; Masaram Gondi
11D60..11DAF; Gunjala Gondi
11EE0..11EFF; Makasar
11FB0..11FBF; Lisu Supplement
11FC0..11FFF; Tamil Supplement
12000..123FF; Cuneiform
12400..1247F; Cuneiform Numbers and Punctuation
12480..1254F; Early Dynastic Cuneiform
12F90..12FFF; Cypro-Minoan
13000..1342F; Egyptian Hieroglyphs
13430..1343F; Egyptian Hieroglyph Format Controls
14400..1467F; Anatolian Hieroglyphs
16800..16A3F; Bamum Supplement
16A40..16A6F; Mro
16A70..16ACF; Tangsa
16AD0..16AFF; Bassa Vah
16B00..16B8F; Pahawh Hmong
16E40..16E9F; Medefaidrin
16F00..16F9F; Miao
16FE0..16FFF; Ideographic Symbols and Punctuation
17000..187FF; Tangut
18800..18AFF; Tangut Components
18B00..18CFF; Khitan Small Script
18D00..18D7F; Tangut Supplement
1AFF0..1AFFF; Kana Extended-B
1B000..1B0FF; Kana Supplement
1B100..1B12F; Kana Extended-A
1B130..1B16F; Small Kana Extension
1B170..1B2FF; Nushu
1BC00..1BC9F; Duployan
1BCA0..1BCAF; Shorthand Format Controls
1CF00..1CFCF; Znamenny Musical Notation
1D000..1D0FF; Byzantine Musical Symbols
1D100..1D1FF; Musical Symbols
1D200..1D24F; Ancient Greek Musical Notation
1D2E0..1D2FF; Mayan Numerals
1D300..1D35F; Tai Xuan Jing Symbols
1D360..1D37F; Counting Rod Numerals
1D400..1D7FF; Mathematical Alphanumeric Symbols
1D800..1DAAF; Sutton SignWriting
1DF00..1DFFF; Latin Extended-G
1E000..1E02F; Glagolitic Supplement
1E100..1E14F; Nyiakeng Puachue Hmong
1E290..1E2BF; Toto
1E2C0..1E2FF; Wancho
1E7E0..1E7FF; Ethiopic Extended-B
1E800..1E8DF; Mende Kikakui
1E900..1E95F; Adlam
1EC70..1ECBF; Indic Siyaq Numbers
1ED00..1ED4F; Ottoman Siyaq Numbers
1EE00..1EEFF; Arabic Mathematical Alphabetic Symbols
1F000..1F02F; Mahjong Tiles
1F030..1F09F; Domino Tiles
1F0A0..1F0FF; Playing Cards
1F100..1F1FF; Enclosed Alphanumeric Supplement
1F200..1F2FF; Enclosed Ideographic Supplement
1F300..1F5FF; Miscellaneous Symbols and Pictographs
1F600..1F64F; Emoticons
1F650..1F67F; Ornamental Dingbats
1F680..1F6FF; Transport and Map Symbols
1F700..1F77F; Alchemical Symbols
1F780..1F7FF; Geometric Shapes Extended
1F800..1F8FF; Supplemental Arrows-C
1F900..1F9FF; Supplemental Symbols and Pictographs
1FA00..1FA6F; Chess Symbols
1FA70..1FAFF; Symbols and Pictographs Extended-A
1FB00..1FBFF; Symbols for Legacy Computing
20000..2A6DF; CJK Unified Ideographs Extension B
2A700..2B73F; CJK Unified Ideographs Extension C
2B740..2B81F; CJK Unified Ideographs Extension D
2B820..2CEAF; CJK Unified Ideographs Extension E
2CEB0..2EBEF; CJK Unified Ideographs Extension F
2F800..2FA1F; CJK Compatibility Ideographs Supplement
30000..3134F; CJK Unified Ideographs Extension G
E0000..E007F; Tags
E0100..E01EF; Variation Selectors Supplement
F0000..FFFFF; Supplementary Private Use Area-A
100000..10FFFF; Supplementary Private Use Area-B
"""
    for line in blockstxt.splitlines():
        try:
            (r, block) = line.split('; ')
            (start, end) = r.split('..')
            start = int(start, 16)
            end = int(end, 16)
            if (start <= x) and (x <= end):
                return block

        except ValueError: continue

    # If we get here, then no block matched.
    return ""

def compileit(s):
    """Compile the regular expresion in s, or die trying.
    This is for debugging of our regex generation, not for speed.
    """
    global ucd, args
    if debug:
        debugprint(s)
        m=search(s,ucd)
        if not m: debugprint("Uncompiled, definitely not in ucd")
    try:
        compile(s)
    except error as e:
        err("Error parsing regex: '%s'" % ".*".join(args.characters))
        err(e)
        exit(3)

def isprint(c):
    # Given a category 'c', return True if it is printable.
    # We presume the only non-printable category is 'C' (control).
    # However, technically, much of category 'Z' (spaces) is non-printable.

    # Side note: Python is silly and regex clauses that match an empty
    # string are set to None instead of ''. That's why we doublecheck
    # that category is not None.
    return c and not c.startswith('C')

def iscombining(c):
    # Given a category 'c', return True if it is a combining character.
    # We presume all combining characters are in category 'M' and vice-versa.
    if debug:
        debugprint ("Category is <" + str(c) + ">")  

    return c and c.startswith('M')

def printmatches(s):
    foundone=None
    for m in finditer(s, ucd):
        if printonematch(m):
            foundone=True

    # Return True if a match was found
    return foundone

def printonematch(m):
    category=""
    foundone=None
    if m:
        g=m.groupdict()
        debugprint(g)

        # Default to an empty string for missing regex keys.
        g=defaultdict(str, g)

        # What is the Pythonic way to do this?
        if not g["hex"]: g["hex"]=g["hextwo"]
        if not g["hex"]: g["hex"]=g["hexthree"]
        cphex=g["hex"]
        if not g["name"]: g["name"]=g["desctwo"]
        if not g["name"]: g["name"]=g["namethree"]
        name=g["name"]
        if not g["category"]: g["category"] = g["categorytwo"]
        if not g["category"]: g["category"] = g["categorythree"]
        category = g["category"]
        if not g["alias"]: g["alias"]=g["aliastwo"]
        if not g["alias"]: g["alias"]=g["aliasthree"]
        alias=g["alias"]

        # Because Python doesn't have AWK's concept of fields, there's
        # a chance the user passed in a regex that crossed into
        # another field. For example: ugrep 'U+.*' would also match
        # semicolons. To handle that, we try to sanitize the data by
        # removing any semicolons and any trailing data.
        cphex = cphex.split(';')[0]
        name = name.split(';')[0]
        category = category.split(';')[0]
        alias = alias.split(';')[0]

        try:
            codepoint=int(cphex, 16)
        except ValueError: 
            debugprint("BUG: Could not convert", cphex, "to integer.")
            return False

        c=chr(codepoint)
        foundone=True
        print("   ", end='') 	# Indent for MacOS
        maybe_open_box(codepoint, name, category)
        if not isprint(category): c="\ufffd" # "Replacement Character"
        if iscombining(category): print('\u25cc', end='') # "Dotted circle"
        if should_box(codepoint, name, category):
            try: c=curses.black_on_yellow(c)
            except NameError: pass
        print(c, end='')
        maybe_close_box(codepoint, name, category)
        print(end='\t')
        print ("U+" + cphex, end='\t')
        print(name, end='')
        if (len(alias) and isdifferent(name, alias)):
            print (" (" + alias.lower() + ")", end='')
        for alias2 in lookup_namealias(cphex):
            if (len(alias2) and isdifferent(alias, alias2)):
                print (" (" + alias2 + ")", end='')

        print("", flush=True)
        # If -l, show list of installed fonts which contain the glyph.
        if args.list_fonts:  show_fonts( cphex, category )

    return foundone

def maybe_open_box(*args):
    """Display the next character differently if it is whitespace or has
    some unusual meaning. This is similar to the Unicode charts which
    put some characters in dashed boxes, but for ugrep we mostly only
    care about marking the text cell should the user wish to copy some
    invisible glyph from the terminal.

    """

    if should_box(*args):
        print("\b[", end='')

def maybe_close_box(*args):
    """Display the previous character differently and disable any
    formatting changes (such as color) set by maybe_open_box."""

    if should_box(*args):
#        print('\u20DE', end='') # "Enclosing box"	# Unnecessary and ugly
        print(']', end='')

def should_box(x, name, category):
    """Input: codepoint x, character name, and category. return true if a
    box of some sort should be drawn around the character to distinguish it.
    This is mostly handy for invisible whitespace which is otherwise
    impossible to cut and paste.
    

    Similar to the "Dashed Box Convention" in the Unicode Charts,
    explained at https://www.unicode.org/versions/Unicode12.0.0/ch24.pdf#G8175.

    # As of version 12.1 it's used on 
    for y in ("0000-0020", "007F-00A0", "00AD", "034F", "0600-0605",
              "061C", "06DD", "070F", "08E2", "0CF1-0CF2", "0D4E",
              "0F0C", "1039", "115F-1160", "17B4-17B5", "17D2",
              "180B-180F", "1A60", "1BAB", "1CF5-1CF6", "2000-200F",
              "2011", "2028-202F", "205F-2064", "2066-206F", "2D7F",
              "2E3A-2E3B", "3000", "303E", "3164", "AAF6",
              "FE00-FE0F", "FEFF", "FFA0", "FFF9-FFFB", "10A3F",
              "11003-11004", "1107F", "110BD", "110CD", "111C2-111C3",
              "11A3A", "11A47", "11A84-11A89", "11A99", "11D45-11D46",
              "11D97", "13430-13438", "16F8F-16F92", "1BC9D",
              "1BCA0-1BCA3", "1D159", "1D173-1D17A", "1DA9B-1DA9F",
              "1DAA1-1DAAF", "1F1E6-1F1FF", "E0001", "E0020-E007F",
              "E0100-E01EF"):

        try:
            (start, end) = y.split('-')
        except ValueError:
            start = y
            end = start

        if x>=int(start, 16) and x<=int(end, 16): return True

    """

    # Do not box up the replacement character
    if category and not isprint(category): return False

    # Box up non-zero width space.
    if category and category.startswith('Z'):
        return True

    return False

def show_fonts(x, category=None):

    """Given a hexadecimal string representing a Unicode code point, 
       list every installed font that defines a glyph at that code point."""
    
    # Normalize whatever they put in. 'U+d0E' to '0D0E'
    x=x.lstrip("Uu+0Xx").upper().zfill(4)

#    try: fonts = getoutput("fc-list -f '%{family[0]}\t%{style[0]}\t%{file}\n' :charset=" + x + " | sort -u")

    # Find fonts that contain x, remove dupes, and put back in original order.
    # (Dupes often caused by multiple point sizes of a bitmap font). 
    fclistcmd="fc-list :charset=" + x
    fclistcmd+=" -f '%{family[0]}\t%{style[0]}\t%{file}\n'"
    fclistcmd+=" | cat -n | sort -k2,3 -u | sort -n | cut -f2-"

    try: fonts = getoutput(fclistcmd)
    except FileNotFoundError: print(""); return 	# No FontConfig
    except KeyboardInterrupt: exit(1) 			# Exit on ^C

    
    #debugprint(fonts)		# List of every matching font.
    
    if debug: count=0
    for f in fonts.splitlines():
        try: (family, style, filename) = f.split('\t')
        except: continue

        if debug:
            count=count+1
            debugprint(count, filename, family, style)

        # Don't print the style if it is just "Regular"
        if style == "Regular" or style == "Roman" or style == "Book":
            style = ""

        # If we're not doing a long listing, then skip variant styles
        if not args.list_fonts_long:
            if style != "": continue

        famstr="%(family)s %(style)s" % locals()
        
        # Text positioning is hairy due to variable text and font size
        if args.fonts_scale <2: 	# The usual case:
            double_text=False
        else:                           # Alternately, with double size text: 
            double_text=True

        # Some terminals cannot handle double-size text
        if args.never_double_text or not isatty(1):
            double_text=False

        # Disable double-size text when debugging (at least for now)
        if debug: double_text=False

        if not double_text:
            famstr = '\t\t' + famstr	# 	Two tabs, sixteen spaces
        else:
            famstr = '\t' + famstr      # 	One double-size tab

        if isatty(1):
            # Writing to a screen? Truncate text if it would wrap.
            (rows, cols) = get_rows_cols()
            if double_text: cols = int(cols/2)
            if len(famstr.expandtabs()) >= cols:
                famstr = famstr.expandtabs()[:cols-1]

        famstr = famstr + '\r'

        if is_sixel_capable(): 
            # If terminal can show sixel graphics, do the following:
            # 1. If fonts_scale >=2, double the size of the the fontname
            # 2. Position the font name based on the fontscale
            # 3. Render a sixel image of the glyph in the font

            esc=chr(0x1b)
            try:
                previous_line=curses.cuu(1)
            except NameError:
                previous_line = esc + '[A' 	# Just presume VT100

            # Escape sequence to print double size text (DECDHL)
            # (Previous line is so we'll be positioned for sixels)
            if double_text:
                famstr = esc + '#3' + famstr + '\n' \
                    + esc + '#4' + famstr + previous_line

            # Align text lower for 4x, 8x sized font glyphs
            if double_text and args.fonts_scale >= 4:
                famstr = '\n' + famstr + previous_line
            if double_text and args.fonts_scale >=8:
                famstr = '\n' + famstr + previous_line

        print(famstr, end='')

        if not is_sixel_capable():
            print("")           # stdout is not a tty, so  send a newline
        else:
	    # stdout is a terminal, show the glyph as sixels
            (foreground, background) = get_term_colors()
            (width, height) = get_cell_size()   # Character cell font size
            height = args.fonts_scale * height  # Increase size of font (maybe).
            if not double_text:
                width=(16-1)*width 	# Leave one space before fontname
            else:
                width=(16-2)*width  	# Leave one double-wide space
            debugprint("Font scaling factor: %g" % (args.fonts_scale))
            debugprint("Font canvas size: %g x %g" % (width, height))

            codepoint=chr(int(x,16))

            # If it's a combining character, add a dotted circle to combine with
            if iscombining(category): codepoint='\u25cc'+codepoint

            command=[ "convert",
                      "-size",
                      "%(width)sx%(height)s" % locals(),
                      "xc:" + "%(background)s" % locals(),
                      "-fill", "%(foreground)s" % locals(),
                      "(", 
                      "-background", "none",
                      "-font", "%(filename)s" % locals(),
                      "label:" + codepoint,
                      "-trim",
                      ")",
                      "-gravity", "east",
                      "-compose", "over",
                      "-composite",
                      "+dither",  
                      "-colors", "4", 	# Faster sixels by reducing colors
                      "sixel:-" ]
            if debug:
                debugprint(*["'" + c + "'" for c in command], flush=True)

            try: output=run(command, capture_output=True)
            except FileNotFoundError: print(""); continue 	# No ImageMagick
            except KeyboardInterrupt: exit(1)                   # Exit on ^C
            except ValueError:                                  # Handle NULL
                if double_text: print("") 
                print("", flush=True)
                continue

            if output.returncode != 0:
                # Oops, font did not render.
                if double_text: print("") 
                print("", flush=True)
                continue

            # Workaround ImageMagick bug by deleting graphics newline (-) at end
            output.stdout.replace(b'-\x1b\\', b'\x1b\\') 

            print(flush=True, end='')   # Send any pending output to stdout

            try: write(1, output.stdout) 	# Print raw bytes to stdout
            except KeyboardInterrupt:		# Catch ^C
                exit(1)                         # Sixels stopped by cleanup()

def is_sixel_capable():
    "Return true if the terminal can show sixel graphics"

    try:
        if is_sixel_capable.cached: return is_sixel_capable.cached
    except AttributeError:
        is_sixel_capable.cached=False

    # If stdin or stdout is not a terminal, don't send sixels
    if not args.pipe_sixels and ( not isatty(0) or not isatty(1) ):
        is_sixel_capable.cached=False
        return False

    # check DA to see if sixels are supported
    results=terminal_query_list('\x1B[c')
    if "4" in results:
        is_sixel_capable.cached=True
        return True

    is_sixel_capable.cached=False
    return False

def get_rows_cols():
    # Return number of (rows, columns) in current terminal window
    # We use termios ioctl as it is dead simple.
    #
    # We cache rows/cols, but perhaps SIGWINCH ought to reset .cached = None.
    try:
        if get_rows_cols.cached: return get_rows_cols.cached
    except AttributeError:
        get_rows_cols.cached=None

    import array, fcntl, termios
    buf = array.array('H', [0, 0, 0, 0])   	# H == unsigned int
    if isatty(0):
        fcntl.ioctl(0, termios.TIOCGWINSZ, buf)
    (rows, cols, x, y) = buf
    if rows==0: rows=24
    if cols==0: cols=80
    get_rows_cols.cached = (rows,cols)
    return (rows,cols)

def get_cell_size():
    """Inquire from the terminal what its character cell size is.
    Returns a tuple (width, height) in pixels. 
    """

    # XXX Shoud we bother to catch SIGWINCH when the font changes?
    #     (Probably not. User can just run ugrep again.)
    try:
        if get_cell_size.cached: return get_cell_size.cached
    except AttributeError:
        get_cell_size.cached=None

    # Ideally, we use an Esc seq to ask terminal what its char cell size is.
    output = terminal_query_list('\x1B[16t')
    if len(output) >= 3:
        try:
            get_cell_size.cached = ( int(output[2]), int(output[1]) )
            return get_cell_size.cached
        except:
            # Response got mangled somehow. Ignore it.
            pass
        
    # Didn't work, so try calculating cell size from geometry.
    # We get geometry in three ways: 

    # First, TIOCGWINSZ ioctl. It's simple but x,y are inaccurate.
    import array, fcntl, termios
    buf = array.array('H', [0, 0, 0, 0]) 	# H == unsigned int
    if isatty(0):
        fcntl.ioctl(0, termios.TIOCGWINSZ, buf)
    # Beware of the stupid output order: y x x y.   --Egmont Koblinger
    (rows, cols, x, y) = buf;

    # Second, XTSMGRAPHICS escape sequence gives the best x,y accuracy.
    output = terminal_query_list('\x1B[?2;1;0S')
    if output:
        try:
            (x, y) = ( int(output[2]), int(output[3]) )
        except ValueError:
            # Escape sequence response got mangled somehow. Ignore it.
            pass
    else:
        # Third, dtterm's x,y are (usually) better than TIOCGWINSZ.
        output = terminal_query_list('\x1B[14t')
        if output:
            try:
                (x, y) = ( int(output[2]), int(output[1]) )
            except ValueError:
                pass

    # VT340 on serial line would still have x and y set to zero.
    if x==0: x=800
    if y==0: y=480

    # TIOCGWINSZ is reliable for rows and cols, but just in case.
    if rows==0: rows=24
    if cols==0: cols=80

    get_cell_size.cached = ( floor(x/cols), floor(y/rows) )

    debugprint("rows: %(rows)s\tcols: %(cols)s\tx: %(x)s\ty: %(y)s" % locals())
    debugprint("charcell w: %d\th: %d" % (floor(x/cols), floor(y/rows) ))
    return get_cell_size.cached

def get_term_colors():
    """Returns a terminals colors as a tuple (foreground, background)
    Handles reversed video by checking mode 5.
    """
    # fg = read -r -p $'\033]10;?\007' -d$'\007'
    # bg = read -r -p $'\033]11;?\007' -d$'\007'

    # Example output from esc sequence:
    # read -r -p $'\033]11;?\007' -d$'\007'
    # ^[]11;rgb:1a1a/1a1a/1a1a^G
    
    try:
        if get_term_colors.cached: return get_term_colors.cached
    except AttributeError:
        # Use VT340 colors by default if detection fails
        get_term_colors.cached= ("gray48", "black") 

    output = terminal_query_list('\x1B]10;?\x1B\\')
    if output:
        # output is of the form: ['10', 'rgb:1abc/2def/3456']
        # but we want just "#1abc2def3456
        fg=output[1].replace('rgb:', '#').replace('/', '')

        output = terminal_query_list('\x1B]11;?\x1B\\')
        if output:
            bg=output[1].replace('rgb:', '#').replace('/', '')

            # Check for reverse video (DECSCNM) i.e., $'\e[?5h' light background
            output = terminal_request_mode(5)
            if output == "set" or output == "permanently set":
                get_term_colors.cached=(bg, fg) # Reversed video
            else:
                get_term_colors.cached=(fg, bg) # Normal (dark) background

    return get_term_colors.cached

def isdifferent(a, b):
    """Given two strings A & B, determine if B is different enough from A
       that we don't need to print it as well. "Enough" means "adds
       significant new information". For example, the following would
       return False:
    
       "LATIN CAPITAL LETTER A WITH GRAVE", "LATIN CAPITAL LETTER A GRAVE"

       Note that order matters. For example,

       "BROKEN BAR", "BROKEN VERTICAL BAR" 	Should return True
       "BROKEN VERTICAL BAR", "BROKEN BAR" 	Should return False
    """
    a=a.upper()
    b=b.upper()

    if a == b:
        return False
    if a == b.replace(" DIGIT ", " "):
        return False
    if a == b.replace("FORMS ", "BOX DRAWINGS "):
        return False
    if a == b.replace("GRAPHIC ", "SYMBOL "):
        return False
    if a == b.replace("GLYPH ", "PRESENTATION FORM "):
        return False
    if a == b.replace("CENTER", "CENTRE"):
        return False
    if a == b.replace("CENTERED", "CENTRED"):
        return False
    if a == b.replace("SQUARED ", "SQUARE "):
        return False

    b=b.replace(" ", ".*")
    try:
        m=search(b, a)
        if m:
            return False
        else:
            return True
    except:
        # regex should never fail, but if it does, we don't care.
        return True


    # if a.replace("WARDS ", " ") == b:
    #     return False
    # if a.replace("S ", " ") == b:
    #     return False
    # if a.rstrip("S") == b:
    #     return False
    
    return True

def loaducd():
    "Find the UnicodeData.txt file and load it up into ucd global variable."

    global ucd
    ucd=None
    ucdplaces=( "UnicodeData.txt",
                "~/.local/share/unicode/UnicodeData.txt",
                "/usr/local/share/unicode/UnicodeData.txt",
                "/usr/share/unicode/UnicodeData.txt"
    )
    for f in ucdplaces:
        f=expanduser(expandvars(f)) 		# Python's open() is silly
        try:
            ucd=open(f).read()
            debugprint("Found data file at " + f)
        except:
            continue

    # Sanity check: did we find the UnicodeData.txt file?
    if ucd == None:
        eprint("""\
    Could not find UnicodeData.txt in:

%s
    On most GNU/Linux systems your package manager can install it easily.
    For example:  apt install unicode-data.

    Alternately, you can grab it via wget like so:

        mkdir -p ~/.local/share/unicode
	cd ~/.local/share/unicode
        wget ftp://ftp.unicode.org/Public/UNIDATA/UnicodeData.txt
""" %
               ("".join(["    " + s + "\n" for s in ucdplaces])))
        exit(2)

    if debug:
        debugprint("Number of Unicode chars: %d" % len(ucd.splitlines()))

    return ucd


readings = None
def loadreadings():
    """Find the Unihan_Readings.txt file and load it up into the
    'readings' global variable. Note that the file may be compressed
    with bzip2. 

    Unicode distributes it within a .zip file, but we do not yet
    handle reading it directly from that file.

    """

    global readings
    if readings != None: return readings 	# Only search once.

    ucdplaces=( "",
                "~/.local/share/unicode/",
                "/usr/local/share/unicode/",
                "/usr/share/unicode/"
    )
    for f in ucdplaces:
        if readings: break
        f=f+"Unihan_Readings.txt"   	# Plain text file, not compressed.
        f=expanduser(expandvars(f)) 	# Python's open() is silly
        try:
            readings=open(f).read()
            debugprint("Found data file at " + f)
        except FileNotFoundError:
            debugprint("File not found at " + f)
            continue

    if readings == None:
        # Plain text not found. Maybe it is compressed?
        try:
            import bz2
            for f in ucdplaces:
                if readings: break
                f=f+"Unihan_Readings.txt.bz2"	# bz2 compressed
                f=expanduser(expandvars(f)) 	# Python's open() is silly
                try:
                    readings=bz2.open(f).read()
                    debugprint("Found data file at " + f)
                except FileNotFoundError:
                    debugprint("File not found at " + f)
                    continue
        except ImportError:
            debugprint("Could not import bz2. Skipped looking for Unihan_Readings.txt.bz2.")
            pass

    # Sanity check: did we find the Unihan_Readings.txt file?
    if readings == None:
        readings = False        # Don't check again for the file
        debugprint("""\
    Could not find Unihan_Readings.txt in:

%s
    On most GNU/Linux systems your package manager can install it easily.
    For example:  apt install unicode-data.

    Alternately, you can grab it via wget like so:

        mkdir -p ~/.local/share/unicode
	cd ~/.local/share/unicode
        wget ftp://ftp.unicode.org/Public/UNIDATA/Unihan.zip
        unzip Unihan.zip
""" %
            ("".join(["    " + s + "Unihan_Readings.txt" + "\n" for s in ucdplaces])))

    return readings


namealiases = None
def loadnamealiases():
    """Find the NameAliases.txt file and load it up into the 'namealiases'
    global variable. This is useful for a handful of aliases which
    should have been in UnicodeData.txt but are not. 

    Unfortunately, much of the file is redundant or unhelpful. For
    example, it has abbreviations of nearly everything, whether it is
    useful (ZWJ) or not (VS256). Most of the aliases which might be
    helpful also exist in the alias field in UnicodeData.txt, and thus
    already used by ugrep.

    In order to not print redundant information, it would be best to
    double check that the namealias is different enough from the
    standard alias. (See the isdifferent() function).

    """

    global namealiases
    if namealiases != None: return namealiases 	# Only search once.

    ucdplaces=( "",
                "~/.local/share/unicode/",
                "/usr/local/share/unicode/",
                "/usr/share/unicode/"
    )
    for f in ucdplaces:
        f=f+"NameAliases.txt"   	# Plain text file
        f=expanduser(expandvars(f)) 	# Python's open() is silly
        try:
            namealiases=open(f).read()
            debugprint("Found data file at " + f)
            break
        except FileNotFoundError:
            debugprint("File not found at " + f)
            continue

    # Sanity check: did we find the NameAliases.txt file?
    if namealiases == None:
        namealiases = False        # Don't check again for the file
        debugprint("""\
    Could not find NameAliases.txt in:

%s
    On most GNU/Linux systems your package manager can install it easily.
    For example:  apt install unicode-data.

    Alternately, you can grab it via wget like so:

        mkdir -p ~/.local/share/unicode
	cd ~/.local/share/unicode
        wget ftp://ftp.unicode.org/Public/UNIDATA/NameAliases.txt
""" %
            ("".join(["    " + s + "NameAliases.txt" + "\n" for s in ucdplaces])))

    return namealiases


def makeregexcharname(words):
    "Create a regular expression to search for a character name"

    # Format of ucd: each character is on a separate line.
    # Each line is fifteen columns separated by semicolons.
    # We only care about columns 1, 2, 3 and 11
    #      1: Code value
    #      2: Character name
    #      3: General category
    #     11: ISO 10646 comment field (usually an alias)
    #
    # For example:
    #     002F;SOLIDUS;Po;0;CS;;;;;N;SLASH;;;;
    #
    # See Tech Report 44 for more details.
    #
    # XXX Todo: maybe look up in table of confusable entities.

    # combine and quote the command line arguments so we can use them in
    # verbose regex. Also, allow the user to use ^ and $, just like in awk
    # to refer to the beginning and end of the field instead of line.
    if type(words) is list:
        words=["("+x+")" for x in words] # wrap in parens to fix alternation
        arghs=".*".join(words)           # search terms can have junk between
    else:
        arghs="(" + "".join(words) + ")"
    debugprint("arghs is", arghs)

    arghs=sub(r"(\s)", r"\\\1", arghs)     # quote whitespace for verbose regex
    arghs=sub(r"^\(\^", r"(?<=;)(", arghs) # ^ matches semicolon before field 
    arghs=sub(r"\$\)$", r"(?=;))", arghs)  # $ matches semicolon after field 

    # s is a search regex for field 2 (name) based on the command line arguments
    s=r"""
        ^(?P<hex>[^;\n]*)		# first field is hexadecimal codepoint
        ;(?P<name>[^;\n]*
        """ + arghs + """		# field 2 (charactername) matches?
        [^;\n]*) 		       
        ;(?P<category>[^;\n]*)		# third field is general category 
        (;[^;\n]*){7}			# skip next 7 fields
        ;(?P<alias>[^;\n]*)		# field 11 is comment/alias
        (;[^;\n]*){4}$			# line ends with four more fields
    """

    # # r is a search regex for field 11 (alias)
    r=r"""
        ^(?P<hextwo>[^;\n]*)            # first field is hexadecimal codepoint
        ;(?P<desctwo>[^;\n]*)           # second field is charactername
        ;(?P<categorytwo>[^;\n]*)       # third field is general category 
        (;[^;\n]*){7}                   # skip next 7 fields
        ;(?P<aliastwo>[^;\n]*(
        """ + arghs + r"""		# field 11 (alias) matches?
        )[^;\n]*)
        (;[^;\n]*){4}$                  # line ends with four more fields
        """

    s=s+"|"+r                   # Search for either name or alias
    s="(?mix)"+s                # multiline, case insensitive, verbose
    return s

def makeregexcodepoint(cp):
    "Make a regular expression to search for a hexadecimal codepoint"

    # User specified a codepoint, e.g., "U+23fb", "0x23FB", "[0-9A-F]{2}"
    # Normalize whatever they put in into "23FB"
    debugprint("given codepoint was", cp)
    cp=cp.upper()
    if cp.find('0X') == 0:  cp=cp.replace('0X', '')
    cp='0*' + cp.lstrip("U+").zfill(4)
    debugprint("codepoint we'll search for is", cp)

    # s is a search regex for field 1 (hexadecimal)
    s=r"""
        ^(?P<hexthree>"""+cp+""")	# first field is hex codepoint
        ;(?P<namethree>[^;\n]*)		# second field is charactername
        ;(?P<categorythree>[^;\n]*)	# third field is category
        (;[^;\n]*){7}			# skip next 7 fields
        ;(?P<aliasthree>[^;\n]*)	# field 11 is comment/alias 
        (;[^;\n]*){4}$ 			# line ends with four more fields
    """
    s="(?mix)"+s                     # multiline, case insensitive, verbose
    return(s)


def makeregexeither(wordsorcp):
    """Make a regular expression to search for a hexadecimal codepoint 
     or for a character name / alias."""

    r=makeregexcharname(wordsorcp)

    if (type(wordsorcp) is list) and (len(wordsorcp) == 1):
        wordsorcp=wordsorcp[0]

    if (type(wordsorcp) is list):
        debugprint("Multiple args, so complete search regex is", r)
        return r

    s=makeregexcodepoint(wordsorcp)

    r = r.lstrip('(?mix)')

    debugprint("Complete search regex is " + s + "|" + r)

    return(s + "|" + r)



# XXXX TODO: This is slow for 0..10FFFF
def rangeexpansion(oldargv):
    """Look for ranges "a..z" or "a..z..i" in oldargv[] and expand by
    inserting new elements. The new argv is returned to the caller.

    0..7 	-->	0 1 2 3 4 5 6 7
    abc0..7 	--> 	abc0 abc1 abc2 abc3 abc4 abc5 abc6 abc7
    23b8..ff	-->	23b8 23b9 23ba 23bb ... 23fd 23fe 23ff
    7fff..8     -->     7fff 7ffe 7ffd 7ffc 7ffb 7ffa 7ff9 7ff8
    0..FFFF..1000 --> 	0000 1000 2000 3000 ... D000 E000 F000

    Optional U+ or 0x is allowed: U+0230..0xF.
    Increment i defaults to +1 or -1 as appropriate.
    Range is inclusive of both ends.
    Output prepends 0x at beginning to force hexadecimal lookup.
	    (fdb..ff should not match OGHAM FEATHER MARK)

    DESIGN NOTES ON CURLY BRACE EXPANSION:

    * TL;DR: Avoid curly braces due to quoting confusion with shell.
    * Ranges within curly braces '{' and '}' are minimally supported.
    * Format: preamble{start..end..increment}postscript.
    * Multiple ranges and nested braces are not supported.
    * The shell's brace expansion is inadequate because it is decimal only.
    * ugrep {0..9}000: works because shell expands it.
    * ugrep {0..F}000: works because we expand it.
    * ugrep  {0..10}000: fails because the shell interprets it as decimal.
    * ugrep \{0..10}000: works because it is quoted so ugrep interprets it.
    * ugrep does not handle comma sequences, but the shell does.
    * ugrep {0..F}{0,4,8,C}00: works due to both shell and ugrep, but
      ugrep 0..FFFF..400: works better as it is sorted correctly.
    * Bash interprets an end number with fewer digits differently than ugrep!
    * ugrep \{8000..8}: ugrep counts up from 8000 to 8008
    * ugrep {8000..8}: Bash counts DOWN (!) from 8000 to 0008 (in DECIMAL).
    * ugrep \{8000..0008}: ugrep counts down from 8000 to 0008 (in hex).

    A NOTE ABOUT SLOWNESS. 

    Ranges are slow in the current implementation because ugrep was
    originally written to create a single regexp and then use the
    language's optimized routines to find it within a file. 
    Ranges simply run it over and over. 

    """



    s=r"""(?mix)
	  (U\+?|0?x)?
    	  ((?P<preamble>[0-9A-F]*)(?P<brace>{))?
    	  (?P<start>[0-9A-F]+)
	  \.\.                 # Literal two periods. MANDATORY.
	  (U\+?|0?x)?
	  (?P<end>[0-9A-F]+)
	  (\.\.)?              # Literal two periods. OPTIONAL.
	  (0?x)?
	  (?P<increment>[-0-9A-F]+)?
    	  (}(?P<postscript>[0-9A-F]*))?
    """
    newargv=[]

    for arg in oldargv:
        m=search(s, arg)
        if not m:
            newargv.append(arg)
        else:
            debugprint("Doing range expansion on " + arg)
            g=m.groupdict()
            preamble=g["preamble"]
            start=g["start"]
            end=g["end"]
            increment=g["increment"]
            postscript=g["postscript"]

            if not preamble: preamble=""
            if not postscript: postscript=""
            if not increment: increment="1"

            # 21b0..f is the same as 21b0..21bf
            end=start[:-len(end)]+end
            z=max(len(start), len(end))

            try:
                start = int(start, 16)
                end = int(end, 16)
                increment = int(increment, 16)
                increment = int(copysign(increment, end-start))
                # Range is inclusive
                end = end + int(copysign(1, increment))

                if debug:
                    if preamble or postscript:
                        debugprint("Preamble: '%s', Postscript: '%s'" %
                                   (preamble, postscript))
                        debugprint("Found hex range %x to %x, increment %x" %
                                   (start, end, increment))

                for i in range(start, end, increment):
                    hexits='0x' + preamble + hex(i)[2:].zfill(z) + postscript
                    newargv.append( hexits )
                    debugprint("   added: " + hexits)
            except ValueError:
                newargv.append(arg)

    return(newargv)


def eprint(*args, **kwargs):
    "Print to stderr"
    print(*args, file=stderr, **kwargs)

def err(*args, **kwargs):
    "Print to stderr with program name prefixed"
    eprint(basename(argv[0]) + ": ", end='')
    eprint(*args, **kwargs)

def debugprint(*args, **kwargs):
    "If debug var is True, print to stderr with progname prefix"
    if (debug):
        err(*args, **kwargs)

def cleanup():
    "Clean up mess before exiting due to an error"
    if isatty(2):
        termios.tcdrain(2) 		# Wait for stderr to be printed
    if isatty(1):
        termios.tcflush(1, termios.TCOFLUSH) 	# Discard stdout

    esc=chr(0x1b)
    eprint(esc+"\\", end='')		# String Terminator for sixels (ST)
    eprint(esc+"#5", end='')		# Single width line (DECSWL)
    eprint("\r" + esc + "[J", end='')	# Erase to end of display (ED)


## See man termios(3) for details on tcgetattr
from termios import *
def terminal_query(seq, delimiter=None, timeout=0.2):
    """
    Given an escape SEQuence, and optionally a DELIMITER and a TIMEOUT,
    print SEQ to stderr and read a response from stdin until the
    character DELIMITER is read or TIMEOUT seconds is reached. 
    Input that is read is returned to the calling function. 

    If TIMEOUT is not specified, it default to 0.2.
    If DELIMITER is not specified, it defaults to the last character of SEQ.

    If neither DELIMITER nor SEQ are specified, then "" is returned.

    """
    import sys, copy, posix

    if not delimiter and not seq:
        return ""

    if not seq: seq=""          # Allow simply reading from terminal.

    # Responses usually end with the same character as the request.
    if not delimiter and len(seq)>0:
        delimiter=seq[-1]

    oldmode = tcgetattr(sys.stdin.fileno())

    # tcgetattr returns [iflag, oflag, cflag, lflag, ispeed, ospeed, cc]
    [ iflag, oflag, cflag, lflag, ispeed, ospeed, cc ]  = oldmode

    ## CBREAK MODE: Read byte by byte.
    ## Cbreak is like Raw but allow ^C interrupt signal and does
    ## not clear OPOST (print newlines as carriage return + newline)

    # Do not transmogrify input in any way...
    iflag = iflag & ~(INPCK | ISTRIP | IXON | ICRNL | INLCR | IGNCR)
    # ... except do allow BREAK (^C) to flush queues and send SIGINT
    iflag = (iflag & ~IGNBRK) | BRKINT

    lflag = lflag & ~ICANON     # Noncanonical: read by bytes, not lines
    lflag = lflag & ~ECHO       # Do not echo characters received

    # Clear character size and disable parity checking
    cflag = cflag & ~(CSIZE | PARENB)
    cflag = cflag | CS8        # Set 8-bit character size

    ## Polling read (MIN == 0, TIME == 0)  See termios(3).
    cc[VMIN] = 0                
    cc[VTIME] = 0
    pollingread = copy.deepcopy([ iflag, oflag, cflag, lflag, ispeed, ospeed, cc ])
    
    ## Read with timeout (MIN == 0, TIME > 0)  See termios(3).
    cc[VMIN] = 0                
    cc[VTIME] = int(timeout*10+0.5) # Timeout is in tenths of a second 
    readwithtimeout = copy.deepcopy([ iflag, oflag, cflag, lflag, ispeed, ospeed, cc ])

    # Drain stdin in case there's junk from a prev req in there already. 
    tcsetattr(sys.stdin.fileno(), TCSANOW, pollingread)
    while posix.read(sys.stdin.fileno(), 1024):
        debugprint(".", end='')

    output=""                   # String of output so far.
    c = None                    # Currently read character.
    try:
        # Next read() should timeout if no byte becomes available.
        tcsetattr(sys.stdin.fileno(), TCSANOW, readwithtimeout)

        print(seq, file=stderr, end='', flush=True) # Send Esc seq to terminal

        # Accumulate response in 'output' until delimiter or timeout
        while c != delimiter:
            c = posix.read(sys.stdin.fileno(), 1).decode()
            if c:
                output=output+c
#                debugprint("got", repr(c), "waiting for", repr(delimiter))
            else:
                debugprint("read() returned 0 characters after", repr(seq))
                # Timeout
                break
    finally:
        tcsetattr(sys.stdin.fileno(), TCSANOW, oldmode)

    if (debug):
        if output:
            debugprint("Terminal query received: ", repr(output))

        if (c == delimiter):
            debugprint("Exited on delimiter", repr(c))
        else:
            debugprint("Exited after timeout", repr(c))

    return(output)

def terminal_query_list(seq, delimiter=None, timeout=0.2):
    """Same as terminal_query() above, but returns the results as a list.
    Also removes known extraneous parts, such as Esc [ at start or Esc \ at end.
    """

    output=terminal_query(seq, delimiter, timeout)

    if not output: return output

    # Example: read -r -p $'\033[c' -dc -t .2
    # 		^[[?63;1;2;4;6;9;15;16;22;28c

    if not delimiter and seq and len(seq)>0:
        delimiter=seq[-1]

    output = output.lstrip('\x1B[').lstrip('\x1B]').lstrip('\x1B').lstrip(';')
    output = output.rstrip(delimiter).rstrip('\x1B\\')
    output = output.split(';')

    return output

def terminal_request_mode(n):
    """DECRQM: Request info on a mode by number.

    Mode 3 is DECCOLM (132 column mode)
    Mode 5 is DECSCNM (reverse video mode)
    Mode 7 is DECAWM (autowrap mode)
    Mode 80 is DECSDM (sixel display mode)"""

    status=("not recognized" "set" "reset" "permanently set" "permanently reset")
    output = terminal_query_list('\x1B[?'+str(n)+'$p', delimiter='y')
    if not output: return output

    output=output[1].strip('$')
    return status[int(output)]

######################################################################

import atexit
atexit.register(cleanup)		       	# When exiting, cleanup sixels


# VT340 is Latin-1 ISO8859-1 encoding.
# Python3 defaults to dying horribly on simple things like print('\u2020')
import sys
if sys.stdout:
    sys.stdout.reconfigure(errors='replace') 	# Print a ? instead of dying.


### Run the main routine
try:
    main()
except BrokenPipeError:
    # Ignore non-error errors. For example: 'ugrep -w pi | head'
    sys.stdout = None;
    cleanup()
except KeyboardInterrupt:		# Catch ^C
    cleanup()

atexit.unregister(cleanup)      # Normal exit, no need to cleanup.

debugprint("Normal exit")

# Implementation notes:

# This is a rewrite of b9's AWK ugrep in Python. While AWK makes a lot
# more sense for what this program does (comparing fields based on
# regexps), a rewrite was necessary because GNU awk, while plenty
# powerful, uses \y for word edges instead of \b and that was bugging
# me. Gawk does this for backwards compatibility with historic AWK,
# which is all well and good, but gawk has no way to disable it for
# new scripts.
#
# Switching to Python did have the benefit of allowing more powerful
# Perl-like regexes (not that anyone has requested that).
# 
# One downside is that I needed a huge hairy regex to simply search
# only in a certain field of each line. Maybe there's some Pythonic
# way to do it, but it's not obvious. Perhaps a 2D array?
#
# Also, I took for granted that awk let me use ^ and $ to search for
# the beginning and ending of fields instead of lines. I tried to
# reimplement that in Python, but it's not quite right as it only
# checks the first and last character. For example, ugrep "^x" works,
# but ugrep "(^x)" does not.


# Note: I do not use Python's `unicodedata` module because it is
# insufficient. It allows one to search by character name only if the
# precise name: `unicodedata.lookup("ROTATED HEAVY BLACK HEART BULLET")`.


################################################################################
# General_Category Values from Unicode TR 44, Table 10.                        #
################################################################################
# Abbr         Long                             Description                    #
# Lu   Uppercase_Letter      an uppercase letter                               #
# Ll   Lowercase_Letter      a lowercase letter                                #
# Lt   Titlecase_Letter      a digraphic character, with first part uppercase  #
# Lm   Modifier_Letter       a modifier letter                                 #
# Lo   Other_Letter          other letters, including syllables and ideographs #
# Mn   Nonspacing_Mark       a nonspacing combining mark (zero advance width)  #
# Mc   Spacing_Mark          a spacing combining mark (positive advance width) #
# Me   Enclosing_Mark        an enclosing combining mark                       #
# Nd   Decimal_Number        a decimal digit                                   #
# Nl   Letter_Number         a letterlike numeric character                    #
# No   Other_Number          a numeric character of other type                 #
# Pc   Connector_Punctuation a connecting punctuation mark, like a tie         #
# Pd   Dash_Punctuation      a dash or hyphen punctuation mark                 #
# Ps   Open_Punctuation      an opening punctuation mark (of a pair)           #
# Pe   Close_Punctuation     a closing punctuation mark (of a pair)            #
# Pi   Initial_Punctuation   an initial quotation mark                         #
# Pf   Final_Punctuation     a final quotation mark                            #
# Po   Other_Punctuation     a punctuation mark of other type                  #
# Sm   Math_Symbol           a symbol of primarily mathematical use            #
# Sc   Currency_Symbol       a currency sign                                   #
# Sk   Modifier_Symbol       a non-letterlike modifier symbol                  #
# So   Other_Symbol          a symbol of other type                            #
# Zs   Space_Separator       a space character (of various non-zero widths)    #
# Zl   Line_Separator        U+2028 LINE SEPARATOR only                        #
# Zp   Paragraph_Separator   U+2029 PARAGRAPH SEPARATOR only                   #
# Cc   Control               a C0 or C1 control code                           #
# Cf   Format                a format control character                        #
# Cs   Surrogate             a surrogate code point                            #
# Co   Private_Use           a private-use character                           #
# Cn   Unassigned            a reserved unassigned code point or noncharacter  #
################################################################################

